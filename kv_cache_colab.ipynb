{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ca2c617",
   "metadata": {},
   "source": [
    "\n",
    "# KV Cache Hands-On Notebook\n",
    "This Colab-ready notebook mirrors the experiments from `kv_cache_blog.md`. Run each section to reproduce the results and compare them with the narrative in the blog post.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0be7cd",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Baseline Attention (No Cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b336b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.8.0+cpu\n",
      "NumPy version: 1.26.4\n",
      "Without cache output shape: (1, 4, 512)\n",
      "First token projection sample: [-0.288, -0.1835, -0.0284, 0.6146, 0.0041]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 4.5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "class SimpleAttentionWithoutCache(nn.Module):\n",
    "    \"\"\"Basic attention without KV cache - inefficient but clear\"\"\"\n",
    "    def __init__(self, dim=512, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = dim // n_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, dim)\n",
    "        self.v_proj = nn.Linear(dim, dim)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        Q = self.q_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "        K = self.k_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "        V = self.v_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=scores.device), diagonal=1).bool()\n",
    "        scores = scores.masked_fill(mask, float(\"-inf\"))\n",
    "\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.dim)\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "example_input = torch.randn(1, 4, 512)\n",
    "baseline_model = SimpleAttentionWithoutCache()\n",
    "with torch.no_grad():\n",
    "    baseline_output = baseline_model(example_input)\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(\"Without cache output shape:\", tuple(baseline_output.shape))\n",
    "print(\"First token projection sample:\", [round(x, 4) for x in baseline_output[0, 0, :5].tolist()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a903371f",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Attention with KV Cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c84b4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt output shape: (1, 3, 512)\n",
      "Cache position after prompt: 3\n",
      "Next-token output shape: (1, 1, 512)\n",
      "Cache position after one-step generation: 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class SimpleAttentionWithCache(nn.Module):\n",
    "    \"\"\"Attention with KV cache - much more efficient for generation\"\"\"\n",
    "    def __init__(self, dim=512, n_heads=8, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = dim // n_heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, dim)\n",
    "        self.v_proj = nn.Linear(dim, dim)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.register_buffer(\"k_cache\", torch.zeros(1, n_heads, max_seq_len, self.head_dim))\n",
    "        self.register_buffer(\"v_cache\", torch.zeros(1, n_heads, max_seq_len, self.head_dim))\n",
    "        self.cache_position = 0\n",
    "\n",
    "    def forward(self, x, use_cache=True):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        if use_cache and seq_len == 1:\n",
    "            Q = self.q_proj(x).view(batch_size, 1, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "            K = self.k_proj(x).view(batch_size, 1, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "            V = self.v_proj(x).view(batch_size, 1, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "            pos = self.cache_position\n",
    "            self.k_cache[:, :, pos:pos + 1, :] = K\n",
    "            self.v_cache[:, :, pos:pos + 1, :] = V\n",
    "\n",
    "            K_full = self.k_cache[:, :, :pos + 1, :]\n",
    "            V_full = self.v_cache[:, :, :pos + 1, :]\n",
    "\n",
    "            scores = torch.matmul(Q, K_full.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "            attn_weights = torch.softmax(scores, dim=-1)\n",
    "            attn_output = torch.matmul(attn_weights, V_full)\n",
    "\n",
    "            self.cache_position += 1\n",
    "        else:\n",
    "            Q = self.q_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "            K = self.k_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "            V = self.v_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "            mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1).bool()\n",
    "            scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "            scores = scores.masked_fill(mask, float(\"-inf\"))\n",
    "            attn_weights = torch.softmax(scores, dim=-1)\n",
    "            attn_output = torch.matmul(attn_weights, V)\n",
    "\n",
    "            if use_cache:\n",
    "                self.k_cache[:, :, :seq_len, :] = K\n",
    "                self.v_cache[:, :, :seq_len, :] = V\n",
    "                self.cache_position = seq_len\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.dim)\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "    def clear_cache(self):\n",
    "        self.k_cache.zero_()\n",
    "        self.v_cache.zero_()\n",
    "        self.cache_position = 0\n",
    "\n",
    "cached_model = SimpleAttentionWithCache(dim=512, n_heads=8, max_seq_len=32)\n",
    "cached_model.clear_cache()\n",
    "prompt_tokens = torch.randn(1, 3, 512)\n",
    "with torch.no_grad():\n",
    "    prompt_output = cached_model(prompt_tokens, use_cache=True)\n",
    "print(\"Prompt output shape:\", tuple(prompt_output.shape))\n",
    "print(\"Cache position after prompt:\", cached_model.cache_position)\n",
    "\n",
    "next_token = torch.randn(1, 1, 512)\n",
    "with torch.no_grad():\n",
    "    next_output = cached_model(next_token, use_cache=True)\n",
    "\n",
    "print(\"Next-token output shape:\", tuple(next_output.shape))\n",
    "print(\"Cache position after one-step generation:\", cached_model.cache_position)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cbf665",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Benchmarking Cache Speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b2151f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time without cache: 0.0190s\n",
      "Total time with cache: 0.0186s\n",
      "Speedup: 1.02x\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def benchmark_generation(model_class, seq_length=25, dim=256):\n",
    "    \"\"\"Compare generation time with and without cache.\"\"\"\n",
    "    model = model_class(dim=dim)\n",
    "    model.eval()\n",
    "\n",
    "    times = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(seq_length):\n",
    "            start = time.time()\n",
    "\n",
    "            if hasattr(model, \"clear_cache\"):\n",
    "                if i == 0:\n",
    "                    model.clear_cache()\n",
    "                x = torch.randn(1, 1, dim)\n",
    "                _ = model(x, use_cache=True)\n",
    "            else:\n",
    "                x = torch.randn(1, i + 1, dim)\n",
    "                _ = model(x)\n",
    "\n",
    "            times.append(time.time() - start)\n",
    "\n",
    "    return times\n",
    "\n",
    "no_cache_times = benchmark_generation(SimpleAttentionWithoutCache, seq_length=25, dim=256)\n",
    "with_cache_times = benchmark_generation(SimpleAttentionWithCache, seq_length=25, dim=256)\n",
    "\n",
    "steps = np.arange(1, len(no_cache_times) + 1)\n",
    "plt.figure()\n",
    "plt.plot(steps, np.cumsum(no_cache_times), label=\"Without KV Cache\", linewidth=2)\n",
    "plt.plot(steps, np.cumsum(with_cache_times), label=\"With KV Cache\", linewidth=2)\n",
    "plt.xlabel(\"Token position\")\n",
    "plt.ylabel(\"Cumulative time (s)\")\n",
    "plt.title(\"Generation Time: With vs Without KV Cache\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print(f\"Total time without cache: {sum(no_cache_times):.4f}s\")\n",
    "print(f\"Total time with cache: {sum(with_cache_times):.4f}s\")\n",
    "print(f\"Speedup: {sum(no_cache_times) / sum(with_cache_times):.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431e7c7e",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Memory Footprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3720874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated KV cache memory for a GPT-3 13B style model:\n",
      "Batch size 1: 3840.00 MB (3.75 GB)\n",
      "Batch size 8: 30720.00 MB (30.00 GB)\n",
      "Batch size 16: 61440.00 MB (60.00 GB)\n",
      "Batch size 32: 122880.00 MB (120.00 GB)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_kv_cache_memory(seq_len, n_layers, n_heads, head_dim, batch_size=1, dtype_bytes=2):\n",
    "    \"\"\"Calculate KV cache memory requirements in megabytes.\"\"\"\n",
    "    cache_per_layer = 2 * batch_size * n_heads * seq_len * head_dim * dtype_bytes\n",
    "    total_cache = n_layers * cache_per_layer\n",
    "    return total_cache / (1024 * 1024)\n",
    "\n",
    "gpt3_base = {\n",
    "    \"seq_len\": 2048,\n",
    "    \"n_layers\": 96,\n",
    "    \"n_heads\": 40,\n",
    "    \"head_dim\": 128\n",
    "}\n",
    "\n",
    "print(\"Estimated KV cache memory for a GPT-3 13B style model:\")\n",
    "for batch_size in [1, 8, 16, 32]:\n",
    "    memory_mb = calculate_kv_cache_memory(batch_size=batch_size, **gpt3_base)\n",
    "    print(f\"Batch size {batch_size}: {memory_mb:.2f} MB ({memory_mb / 1024:.2f} GB)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9314f5a5",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Minimal Transformer with KV Cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13693185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt token IDs: [175, 419, 281, 170, 137]\n",
      "Generated token IDs: [175, 419, 281, 170, 137, 147, 343, 45, 368, 445, 128, 298, 322, 245, 50, 280, 36, 221, 51, 105]\n",
      "New tokens generated: 15\n",
      "Generation time: 0.044s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class SimpleTransformerWithCache(nn.Module):\n",
    "    \"\"\"A minimal transformer for demonstration.\"\"\"\n",
    "    def __init__(self, vocab_size=50257, dim=512, n_heads=8, n_layers=6, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.token_embedding = nn.Embedding(vocab_size, dim)\n",
    "        self.pos_embedding = nn.Embedding(max_seq_len, dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            SimpleAttentionWithCache(dim, n_heads, max_seq_len=max_seq_len) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_final = nn.LayerNorm(dim)\n",
    "        self.lm_head = nn.Linear(dim, vocab_size, bias=False)\n",
    "        self.current_position = 0\n",
    "\n",
    "    def forward(self, input_ids, use_cache=True):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        if use_cache:\n",
    "            start_pos = self.current_position\n",
    "            positions = torch.arange(start_pos, start_pos + seq_len, device=input_ids.device)\n",
    "            self.current_position += seq_len\n",
    "        else:\n",
    "            positions = torch.arange(seq_len, device=input_ids.device)\n",
    "\n",
    "        x = self.token_embedding(input_ids)\n",
    "        x = x + self.pos_embedding(positions).unsqueeze(0)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = x + layer(x, use_cache=use_cache)\n",
    "\n",
    "        x = self.ln_final(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "    def clear_cache(self):\n",
    "        for layer in self.layers:\n",
    "            layer.clear_cache()\n",
    "        self.current_position = 0\n",
    "\n",
    "    def generate(self, prompt_ids, max_length=50, temperature=0.8):\n",
    "        self.eval()\n",
    "        generated = prompt_ids.clone()\n",
    "        self.clear_cache()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(prompt_ids, use_cache=True)\n",
    "\n",
    "            for _ in range(max_length - prompt_ids.shape[1]):\n",
    "                logits_last = logits[:, -1, :] / temperature\n",
    "                probs = torch.softmax(logits_last, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                generated = torch.cat([generated, next_token], dim=1)\n",
    "                logits = self.forward(next_token, use_cache=True)\n",
    "\n",
    "        return generated\n",
    "\n",
    "demo_model = SimpleTransformerWithCache(vocab_size=500, dim=128, n_heads=4, n_layers=2, max_seq_len=64)\n",
    "demo_prompt = torch.randint(0, 500, (1, 5))\n",
    "start = time.time()\n",
    "demo_output = demo_model.generate(demo_prompt, max_length=20, temperature=0.9)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(\"Prompt token IDs:\", demo_prompt.tolist()[0])\n",
    "print(\"Generated token IDs:\", demo_output.tolist()[0])\n",
    "print(f\"New tokens generated: {demo_output.shape[1] - demo_prompt.shape[1]}\")\n",
    "print(f\"Generation time: {elapsed:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d84b7b7",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Multi-Query and Grouped-Query Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27260601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MQA output shape: (2, 5, 256)\n",
      "Standard MHA KV cache: 1024.00 MB\n",
      "MQA KV cache: 32.00 MB (32.0x smaller)\n",
      "GQA KV cache (8 groups): 256.00 MB (4.0x smaller)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MultiQueryAttention(nn.Module):\n",
    "    \"\"\"MQA: shared K,V across all heads.\"\"\"\n",
    "    def __init__(self, dim=512, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = dim // n_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, self.head_dim)\n",
    "        self.v_proj = nn.Linear(dim, self.head_dim)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        Q = self.q_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        K = self.k_proj(x).view(batch_size, seq_len, 1, self.head_dim)\n",
    "        V = self.v_proj(x).view(batch_size, seq_len, 1, self.head_dim)\n",
    "\n",
    "        K = K.expand(-1, -1, self.n_heads, -1).transpose(1, 2)\n",
    "        V = V.expand(-1, -1, self.n_heads, -1).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.dim)\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "def compare_cache_memory(seq_len=2048, n_layers=32, n_heads=32, head_dim=128, dtype_bytes=2):\n",
    "    mha_cache = 2 * n_layers * n_heads * seq_len * head_dim * dtype_bytes\n",
    "    mqa_cache = 2 * n_layers * 1 * seq_len * head_dim * dtype_bytes\n",
    "    n_groups = 8\n",
    "    gqa_cache = 2 * n_layers * n_groups * seq_len * head_dim * dtype_bytes\n",
    "\n",
    "    print(f\"Standard MHA KV cache: {mha_cache / (1024**2):.2f} MB\")\n",
    "    print(f\"MQA KV cache: {mqa_cache / (1024**2):.2f} MB ({mha_cache / mqa_cache:.1f}x smaller)\")\n",
    "    print(f\"GQA KV cache (8 groups): {gqa_cache / (1024**2):.2f} MB ({mha_cache / gqa_cache:.1f}x smaller)\")\n",
    "\n",
    "mqa_demo = MultiQueryAttention(dim=256, n_heads=8)\n",
    "sample_tokens = torch.randn(2, 5, 256)\n",
    "with torch.no_grad():\n",
    "    mqa_output = mqa_demo(sample_tokens)\n",
    "\n",
    "print(\"MQA output shape:\", tuple(mqa_output.shape))\n",
    "compare_cache_memory()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26c7f99",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Sliding Window Cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1efacbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: cache length 1\n",
      "Step 2: cache length 2\n",
      "Step 3: cache length 3\n",
      "Step 4: cache length 4\n",
      "Step 5: cache length 4\n",
      "Step 6: cache length 4\n",
      "Latest K entries: [[3.0, 3.0, 3.0], [4.0, 4.0, 4.0], [5.0, 5.0, 5.0], [6.0, 6.0, 6.0]]\n",
      "Latest V entries: [[-3.0, -3.0, -3.0], [-4.0, -4.0, -4.0], [-5.0, -5.0, -5.0], [-6.0, -6.0, -6.0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class SlidingWindowCache:\n",
    "    \"\"\"Maintain only recent K tokens in cache.\"\"\"\n",
    "    def __init__(self, window_size=1024, dim=512):\n",
    "        self.window_size = window_size\n",
    "        self.k_cache = torch.zeros(window_size, dim)\n",
    "        self.v_cache = torch.zeros(window_size, dim)\n",
    "        self.position = 0\n",
    "\n",
    "    def update(self, k, v):\n",
    "        idx = self.position % self.window_size\n",
    "        self.k_cache[idx] = k\n",
    "        self.v_cache[idx] = v\n",
    "        self.position += 1\n",
    "\n",
    "    def get_cache(self):\n",
    "        if self.position < self.window_size:\n",
    "            return self.k_cache[:self.position], self.v_cache[:self.position]\n",
    "        idx = self.position % self.window_size\n",
    "        k = torch.cat([self.k_cache[idx:], self.k_cache[:idx]], dim=0)\n",
    "        v = torch.cat([self.v_cache[idx:], self.v_cache[:idx]], dim=0)\n",
    "        return k, v\n",
    "\n",
    "window_cache = SlidingWindowCache(window_size=4, dim=3)\n",
    "for step in range(6):\n",
    "    k_vec = torch.full((3,), float(step + 1))\n",
    "    v_vec = torch.full((3,), float(-(step + 1)))\n",
    "    window_cache.update(k_vec, v_vec)\n",
    "    current_k, current_v = window_cache.get_cache()\n",
    "    print(f\"Step {step + 1}: cache length {current_k.shape[0]}\")\n",
    "\n",
    "print(\"Latest K entries:\", [row.tolist() for row in current_k])\n",
    "print(\"Latest V entries:\", [row.tolist() for row in current_v])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a503913",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Dynamic Cache Allocation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60dba634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic cache K shape: (4, 5, 3)\n",
      "Latest timestep keys: [[5.0, 5.0, 5.0], [5.0, 5.0, 5.0], [5.0, 5.0, 5.0], [5.0, 5.0, 5.0]]\n",
      "Cache cleared. Stored entries: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class DynamicKVCache:\n",
    "    \"\"\"Dynamically growing KV cache.\"\"\"\n",
    "    def __init__(self, n_heads, head_dim):\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.k_cache = []\n",
    "        self.v_cache = []\n",
    "\n",
    "    def update(self, k, v):\n",
    "        self.k_cache.append(k)\n",
    "        self.v_cache.append(v)\n",
    "\n",
    "    def get_cache(self):\n",
    "        if not self.k_cache:\n",
    "            return None, None\n",
    "        k = torch.stack(self.k_cache, dim=1)\n",
    "        v = torch.stack(self.v_cache, dim=1)\n",
    "        return k, v\n",
    "\n",
    "    def clear(self):\n",
    "        self.k_cache = []\n",
    "        self.v_cache = []\n",
    "\n",
    "dynamic_cache = DynamicKVCache(n_heads=4, head_dim=3)\n",
    "for step in range(5):\n",
    "    k = torch.full((4, 3), float(step + 1))\n",
    "    v = torch.full((4, 3), float(-(step + 1)))\n",
    "    dynamic_cache.update(k, v)\n",
    "\n",
    "k_stack, v_stack = dynamic_cache.get_cache()\n",
    "print(\"Dynamic cache K shape:\", tuple(k_stack.shape))\n",
    "print(\"Latest timestep keys:\", k_stack[:, -1, :].tolist())\n",
    "dynamic_cache.clear()\n",
    "print(\"Cache cleared. Stored entries:\", len(dynamic_cache.k_cache))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db8a7b6",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Debugging Helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df79ce21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs match between cached and uncached paths.\n",
      "KV cache validation complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def debug_kv_cache(model, input_sequence):\n",
    "    \"\"\"Helper function to debug KV cache issues.\"\"\"\n",
    "    model.clear_cache()\n",
    "    initial_cache = model.layers[0].k_cache.clone()\n",
    "    _ = model(input_sequence[:, :1], use_cache=True)\n",
    "    assert not torch.equal(initial_cache, model.layers[0].k_cache), \"Cache not updating.\"\n",
    "\n",
    "    for i in range(input_sequence.shape[1]):\n",
    "        model.clear_cache()\n",
    "        _ = model(input_sequence[:, :i + 1], use_cache=True)\n",
    "        cache_pos = model.layers[0].cache_position\n",
    "        assert cache_pos == i + 1, f\"Cache position mismatch at {i}: {cache_pos} != {i + 1}\"\n",
    "\n",
    "    model.clear_cache()\n",
    "    with_cache_output = model(input_sequence, use_cache=True)\n",
    "    model.clear_cache()\n",
    "    without_cache_output = model(input_sequence, use_cache=False)\n",
    "\n",
    "    if not torch.allclose(with_cache_output, without_cache_output, rtol=1e-5, atol=1e-5):\n",
    "        diff = (with_cache_output - without_cache_output).abs().max()\n",
    "        print(\"Warning: outputs differ between cached and uncached paths.\")\n",
    "        print(f\"Max difference: {diff.item():.3e}\")\n",
    "    else:\n",
    "        print(\"Outputs match between cached and uncached paths.\")\n",
    "\n",
    "    print(\"KV cache validation complete.\")\n",
    "\n",
    "test_model = SimpleTransformerWithCache(vocab_size=300, dim=64, n_heads=4, n_layers=2, max_seq_len=32)\n",
    "test_sequence = torch.randint(0, 300, (1, 6))\n",
    "debug_kv_cache(test_model, test_sequence)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "kv_cache_colab.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
