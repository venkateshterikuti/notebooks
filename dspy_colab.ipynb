{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc217d0",
   "metadata": {
    "id": "5dc217d0"
   },
   "source": [
    "# DSPy Playground: Companion Notebook\n",
    "\n",
    "This notebook mirrors the narrative in dspy_blog.md so you can execute every experiment while reading the story.\n",
    "\n",
    "**How to use it**\n",
    "\n",
    "1. Run the setup cells below (installation + language model configuration).\n",
    "2. If you have an OpenAI key, set os.environ[\\\"OPENAI_API_KEY\\\"] = \\\"sk-...\\\" before re-running the configuration cell. Without a key, the notebook defaults to a playful simulated model so every block still prints output.\n",
    "3. Keep the blog open side-by-side and tinker with the code to explore variations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cbbef21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-21T13:06:07.690526Z",
     "iopub.status.busy": "2025-09-21T13:06:07.689521Z",
     "iopub.status.idle": "2025-09-21T13:06:17.177224Z",
     "shell.execute_reply": "2025-09-21T13:06:17.176207Z"
    },
    "id": "2cbbef21",
    "outputId": "a66e5b4a-5522-44bf-fd60-bff4d58a21e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install DSPy and supporting libraries\n",
    "!pip install -q dspy-ai fastapi uvicorn transformers accelerate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HjB0N4BAFX7V",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-21T13:06:17.181217Z",
     "iopub.status.busy": "2025-09-21T13:06:17.181217Z",
     "iopub.status.idle": "2025-09-21T13:06:31.104764Z",
     "shell.execute_reply": "2025-09-21T13:06:31.104764Z"
    },
    "id": "HjB0N4BAFX7V",
    "outputId": "1c67f28c-ca7d-4a78-f58d-6436bf000fb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Retrieval-Augmented Generation (RAG) is a natural language processing model that combines elements of both retrieval-based and generation-based approaches to improve the quality of text generation. In RAG, a retrieval component is used to search for relevant information from a large database of text, which is then used to augment the generation process. This allows the model to incorporate factual information and context from the retrieved text into the generated output, resulting in more accurate and coherent text generation. RAG has been shown to outperform traditional generation models in tasks such as question answering and text summarization by leveraging the benefits of both retrieval and generation techniques.']\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"api-key\"\n",
    "lm = dspy.LM(provider=\"openai\", model=\"gpt-3.5-turbo\", max_tokens=1000)\n",
    "response = lm(\"Explain what Retrieval-Augmented Generation (RAG) is.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "GNEE4l38N4RR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-21T13:06:31.111540Z",
     "iopub.status.busy": "2025-09-21T13:06:31.110540Z",
     "iopub.status.idle": "2025-09-21T13:06:31.120622Z",
     "shell.execute_reply": "2025-09-21T13:06:31.120118Z"
    },
    "id": "GNEE4l38N4RR",
    "outputId": "a4bc8787-9e93-4bd8-f330-129fcbf8764b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Retrieval-augmented generation allows institutional investors to access a vast amount of data and information related to ESG risks, enabling them to make more informed investment decisions. By combining retrieval of relevant data with generation of insights and analysis, investors can better understand the potential impact of ESG risks on their portfolios and take proactive measures to mitigate them.']\n"
     ]
    }
   ],
   "source": [
    "lm = dspy.LM(provider=\"openai\", model=\"gpt-3.5-turbo\", max_tokens=512)\n",
    "response = lm(\"In two sentences, explain how retrieval-augmented generation strengthens ESG risk analysis for institutional investors.\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0deccfda",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-21T13:06:31.126930Z",
     "iopub.status.busy": "2025-09-21T13:06:31.125633Z",
     "iopub.status.idle": "2025-09-21T13:06:32.992211Z",
     "shell.execute_reply": "2025-09-21T13:06:32.991196Z"
    },
    "id": "0deccfda",
    "outputId": "43e88d2e-6a36-4422-fb1d-da5fbc202f58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Connected to OpenAI model: gpt-3.5-turbo\n",
      "LM ready: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import types\n",
    "import asyncio\n",
    "import logging\n",
    "import pickle\n",
    "import hashlib\n",
    "import textwrap\n",
    "from datetime import datetime\n",
    "from typing import List, Optional\n",
    "from functools import lru_cache\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import pandas as pd\n",
    "import dspy\n",
    "#from dspy.models import openai\n",
    "#from dspy import openai\n",
    "from dspy.teleprompt import BootstrapFewShot, BootstrapFewShotWithRandomSearch, LabeledFewShot, BootstrapFinetune\n",
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "TRACE_LOG: List[str] = []\n",
    "\n",
    "\n",
    "class EchoLM(dspy.BaseLM):\n",
    "    \"\"\"Fallback LM that fabricates structured outputs so every cell prints.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(model=\"echo-playground\", temperature=0.0, max_tokens=512, cache=False)\n",
    "\n",
    "    def forward(self, prompt, messages=None, **kwargs):\n",
    "        system_content = \"\"\n",
    "        if messages:\n",
    "            for message in messages:\n",
    "                if isinstance(message, dict) and message.get(\"role\") == \"system\":\n",
    "                    system_content = message.get(\"content\", \"\")\n",
    "        output_fields = []\n",
    "        if \"Your output fields are:\" in system_content:\n",
    "            after = system_content.split(\"Your output fields are:\")[1]\n",
    "            before = after.split(\"All interactions\")[0]\n",
    "            # Use simpler string splitting instead of regex\n",
    "            output_fields = [line.strip() for line in before.split('\\n') if line.strip()]\n",
    "        if not output_fields:\n",
    "            output_fields = [\"answer\"]\n",
    "\n",
    "        if messages:\n",
    "            last_message = messages[-1]\n",
    "            if isinstance(last_message, dict):\n",
    "                user_content = last_message.get(\"content\", \"\")\n",
    "            elif hasattr(last_message, \"content\"):\n",
    "                user_content = last_message.content\n",
    "            else:\n",
    "                user_content = str(last_message)\n",
    "        else:\n",
    "            user_content = prompt or \"\"\n",
    "\n",
    "        snippet = (user_content or \"\").replace(\"\\n\", \" \").strip()\n",
    "        if len(snippet) > 140:\n",
    "            snippet = snippet[:137] + \"...\"\n",
    "\n",
    "        lines = []\n",
    "        for field in output_fields:\n",
    "            label = field.replace(\"_\", \" \")\n",
    "            lines.append(f\"[[ ## {field} ## ]]\")\n",
    "            lines.append(f\"Simulated {label} for: {snippet or '<empty>'}\")\n",
    "            lines.append(\"\")\n",
    "        lines.append(\"[[ ## completed ## ]]\")\n",
    "        content = \"\\n\".join(lines)\n",
    "\n",
    "        message = types.SimpleNamespace(content=content, role=\"assistant\")\n",
    "        choice = types.SimpleNamespace(message=message, finish_reason=\"stop\")\n",
    "        return types.SimpleNamespace(\n",
    "            choices=[choice],\n",
    "            usage={\n",
    "                \"prompt_tokens\": len(snippet.split()),\n",
    "                \"completion_tokens\": len(content.split()),\n",
    "                \"total_tokens\": len(snippet.split()) + len(content.split()),\n",
    "            },\n",
    "            model=self.model,\n",
    "        )\n",
    "\n",
    "\n",
    "def configure_language_model() -> bool:\n",
    "    \"\"\"Try to wire up a real LM, then fall back to EchoLM.\"\"\"\n",
    "\n",
    "    openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    model_name = os.getenv(\"OPENAI_MODEL\", \"gpt-3.5-turbo\")\n",
    "\n",
    "    if openai_key:\n",
    "        try:\n",
    "            lm = dspy.LM(provider=\"openai\", model=model_name, temperature=0.7, max_tokens=800)\n",
    "            dspy.settings.configure(lm=lm, trace=TRACE_LOG)\n",
    "            print(f\"âœ… Connected to OpenAI model: {model_name}\")\n",
    "            return True\n",
    "        except Exception as exc:\n",
    "            print(f\"âš ï¸ OpenAI setup failed: {exc}\")\n",
    "\n",
    "    fallback_lm = EchoLM()\n",
    "    dspy.settings.configure(lm=fallback_lm, trace=TRACE_LOG)\n",
    "    print(\"ðŸ”„ Using the simulated EchoLM fallback. Set OPENAI_API_KEY to talk to a real model.\")\n",
    "    return False\n",
    "\n",
    "\n",
    "HAVE_REAL_LM = configure_language_model()\n",
    "print(f\"LM ready: {HAVE_REAL_LM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3d8037d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-21T13:06:33.000581Z",
     "iopub.status.busy": "2025-09-21T13:06:32.999566Z",
     "iopub.status.idle": "2025-09-21T13:06:33.013236Z",
     "shell.execute_reply": "2025-09-21T13:06:33.011465Z"
    },
    "id": "b3d8037d",
    "outputId": "8387d04f-850b-435e-82fb-e37af12eebdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt preview:\n",
      "  Extract the following information from the text:\n",
      "      - Mission or initiative name\n",
      "      - Key commitments (list them)\n",
      "      - Sentiment (positive/negative/neutral)\n",
      "\n",
      "      Text: In March ...\n",
      "Raw response: {\"Mission or initiative name\": \"TRUTHS climate mission\", \"Key commitments\": [\"Calibrate satellite climate observations with traceable radiometry\", \"Sustain a partnership between ESA, the UKSA, and the Canadian Space Agency\", \"Launch on Vega-C in 2030 to provide authoritative greenhouse gas baselines\"], \"Sentiment\": \"positive\"}\n",
      "Parsed output -> {'parsed': {'Mission or initiative name': 'TRUTHS climate mission', 'Key commitments': ['Calibrate satellite climate observations with traceable radiometry', 'Sustain a partnership between ESA, the UKSA, and the Canadian Space Agency', 'Launch on Vega-C in 2030 to provide authoritative greenhouse gas baselines'], 'Sentiment': 'positive'}}\n"
     ]
    }
   ],
   "source": [
    "# The Old Way: Fragile prompt engineering\n",
    "def extract_info_traditional(text):\n",
    "    prompt = f\"\"\"Extract the following information from the text:\n",
    "    - Mission or initiative name\n",
    "    - Key commitments (list them)\n",
    "    - Sentiment (positive/negative/neutral)\n",
    "\n",
    "    Text: {text}\n",
    "\n",
    "    Format your response as JSON.\n",
    "    Make sure to include all fields.\n",
    "    Be concise but comprehensive.\n",
    "    Double-check your JSON formatting.\n",
    "    \"\"\"\n",
    "\n",
    "    response = call_llm(prompt)\n",
    "    # Hope the LLM follows our format...\n",
    "    # Parse JSON (might fail!)\n",
    "    # Handle edge cases manually\n",
    "    return parse_somehow(response)\n",
    "\n",
    "\n",
    "def call_llm(prompt: str) -> str:\n",
    "    \"\"\"Pretend to call an LLM so the classic example prints output.\"\"\"\n",
    "\n",
    "    preview = (prompt[:180] + \"...\") if len(prompt) > 180 else prompt\n",
    "    print(\"Prompt preview:\\n\" + textwrap.indent(preview, \"  \"))\n",
    "    payload = {\n",
    "        \"Mission or initiative name\": \"TRUTHS climate mission\",\n",
    "        \"Key commitments\": [\n",
    "            \"Calibrate satellite climate observations with traceable radiometry\",\n",
    "            \"Sustain a partnership between ESA, the UKSA, and the Canadian Space Agency\",\n",
    "            \"Launch on Vega-C in 2030 to provide authoritative greenhouse gas baselines\"\n",
    "        ],\n",
    "        \"Sentiment\": \"positive\"\n",
    "    }\n",
    "    return json.dumps(payload)\n",
    "\n",
    "\n",
    "def parse_somehow(response: str) -> dict:\n",
    "    print(f\"Raw response: {response}\")\n",
    "    return {\"parsed\": json.loads(response)}\n",
    "\n",
    "\n",
    "sample_text = \"\"\"In March 2024, the European Space Agency formally approved the TRUTHS climate mission, an observatory designed to calibrate satellite measurements of Earth's radiative balance. The programme is led by the UK Space Agency in partnership with the Canadian Space Agency and national metrology laboratories. Its goals include delivering traceable radiometric data for greenhouse gas inventories, validating commercial climate analytics, and accelerating climate model tuning. Launch is targeted for 2030 aboard a Vega-C rocket from French Guiana.\"\"\"\n",
    "\n",
    "sample_traditional = extract_info_traditional(sample_text)\n",
    "print(\"Parsed output ->\", sample_traditional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46a054c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-21T13:06:33.018912Z",
     "iopub.status.busy": "2025-09-21T13:06:33.018912Z",
     "iopub.status.idle": "2025-09-21T13:06:33.036301Z",
     "shell.execute_reply": "2025-09-21T13:06:33.035290Z"
    },
    "id": "46a054c9",
    "outputId": "1b899b7a-9215-4f80-c69c-1f70d274bcd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… BasicQA and DocumentQA signatures registered.\n"
     ]
    }
   ],
   "source": [
    "# Signatures: Defining What, Not How\n",
    "class BasicQA(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factual answers.\"\"\"\n",
    "\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n",
    "\n",
    "\n",
    "class DocumentQA(dspy.Signature):\n",
    "    \"\"\"Answer question based on given context.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"relevant background information\")\n",
    "    question = dspy.InputField(desc=\"question to be answered\")\n",
    "    answer = dspy.OutputField(desc=\"detailed answer based on context\")\n",
    "\n",
    "print(\"âœ… BasicQA and DocumentQA signatures registered.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2b9245c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-21T13:06:33.041303Z",
     "iopub.status.busy": "2025-09-21T13:06:33.041303Z",
     "iopub.status.idle": "2025-09-21T13:06:34.448226Z",
     "shell.execute_reply": "2025-09-21T13:06:34.447214Z"
    },
    "id": "f2b9245c",
    "outputId": "0121e8ef-8db7-4610-b72e-aa85eac1433e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: France\n"
     ]
    }
   ],
   "source": [
    "# Modules: Composable LLM Programs\n",
    "class SimpleQAModule(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_answer = dspy.Predict(BasicQA)\n",
    "\n",
    "    def forward(self, question):\n",
    "        prediction = self.generate_answer(question=question)\n",
    "        return prediction.answer\n",
    "\n",
    "\n",
    "qa = SimpleQAModule()\n",
    "result = qa(question=\"Which country hosts the ITER fusion reactor construction site?\")\n",
    "print(f\"Answer: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76c2f04c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-21T13:06:34.453224Z",
     "iopub.status.busy": "2025-09-21T13:06:34.452224Z",
     "iopub.status.idle": "2025-09-21T13:06:34.666534Z",
     "shell.execute_reply": "2025-09-21T13:06:34.664868Z"
    },
    "id": "76c2f04c",
    "outputId": "43a898c1-7dfa-4736-b1e6-68f563b8d653"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Claim: GridSketch is an orchestration framework for hybrid microgrids that reduces diesel peaker usage by 38% in island grids while maintaining reserve margins during cyclone-driven outages.\n",
      "\n",
      "Methods: machine learning; hierarchical control; field trials; dispatch logs analysis; modular API integration\n",
      "\n",
      "Novelty Score: 7/10\n",
      "\n",
      "Explanation: The research contribution of GridSketch lies in its development as an orchestration framework for hybrid microgrids that effectively reduces diesel peaker usage by 38% in island grids while ensuring reserve margins during cyclone-driven outages. This novel approach combines battery storage, electrolyzers, and adaptive demand forecasting, along with hierarchical control policies and graph neural optimizers trained on dispatch logs. The modular APIs further enhance its adaptability by allowing operators to incorporate market signals and carbon pricing objectives without the need to rewrite controllers.\n",
      "\n",
      "Summary: GridSketch is a novel orchestration framework for hybrid microgrids that effectively reduces diesel peaker usage by 38% in island grids, while ensuring reserve margins are maintained during cyclone-driven outages. The framework utilizes machine learning techniques, hierarchical control policies, and field trials in Barbados, Martinique, and Guadeloupe to achieve this significant reduction. Additionally, its modular APIs allow for easy integration of market signals and carbon pricing objectives without the need for rewriting controllers.\n"
     ]
    }
   ],
   "source": [
    "# Building a Real Application: Research Paper Analyzer\n",
    "class ExtractMainClaim(dspy.Signature):\n",
    "    \"\"\"Extract the main claim or thesis from an academic abstract.\"\"\"\n",
    "\n",
    "    abstract = dspy.InputField(desc=\"academic paper abstract\")\n",
    "    main_claim = dspy.OutputField(desc=\"the primary claim or contribution in one sentence\")\n",
    "\n",
    "\n",
    "class IdentifyMethods(dspy.Signature):\n",
    "    \"\"\"Identify research methods used in the paper.\"\"\"\n",
    "\n",
    "    abstract = dspy.InputField()\n",
    "    methods = dspy.OutputField(desc=\"list of research methods, separated by semicolons\")\n",
    "\n",
    "\n",
    "class AssessNovelty(dspy.Signature):\n",
    "    \"\"\"Assess the novelty of the research contribution.\"\"\"\n",
    "\n",
    "    abstract = dspy.InputField()\n",
    "    main_claim = dspy.InputField()\n",
    "    novelty_score = dspy.OutputField(desc=\"integer from 1 to 10\")\n",
    "    novelty_explanation = dspy.OutputField(desc=\"brief explanation of score\")\n",
    "\n",
    "\n",
    "class GenerateSummary(dspy.Signature):\n",
    "    \"\"\"Generate a structured summary of the research.\"\"\"\n",
    "\n",
    "    abstract = dspy.InputField()\n",
    "    main_claim = dspy.InputField()\n",
    "    methods = dspy.InputField()\n",
    "    novelty_score = dspy.InputField()\n",
    "    summary = dspy.OutputField(desc=\"3-4 sentence summary for a general audience\")\n",
    "\n",
    "\n",
    "class ResearchPaperAnalyzer(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialize our predictors\n",
    "        self.extract_claim = dspy.Predict(ExtractMainClaim)\n",
    "        self.identify_methods = dspy.Predict(IdentifyMethods)\n",
    "        self.assess_novelty = dspy.Predict(AssessNovelty)\n",
    "        self.generate_summary = dspy.Predict(GenerateSummary)\n",
    "\n",
    "    def forward(self, abstract):\n",
    "        # Extract main claim\n",
    "        claim = self.extract_claim(abstract=abstract).main_claim\n",
    "\n",
    "        # Identify methods\n",
    "        methods = self.identify_methods(abstract=abstract).methods\n",
    "\n",
    "        # Assess novelty\n",
    "        novelty_assessment = self.assess_novelty(\n",
    "            abstract=abstract,\n",
    "            main_claim=claim\n",
    "        )\n",
    "\n",
    "        # Generate final summary\n",
    "        summary = self.generate_summary(\n",
    "            abstract=abstract,\n",
    "            main_claim=claim,\n",
    "            methods=methods,\n",
    "            novelty_score=novelty_assessment.novelty_score\n",
    "        ).summary\n",
    "\n",
    "        return dspy.Prediction(\n",
    "            main_claim=claim,\n",
    "            methods=methods,\n",
    "            novelty_score=novelty_assessment.novelty_score,\n",
    "            novelty_explanation=novelty_assessment.novelty_explanation,\n",
    "            summary=summary\n",
    "        )\n",
    "\n",
    "\n",
    "analyzer = ResearchPaperAnalyzer()\n",
    "test_abstract = \"\"\"\n",
    "We present GridSketch, an orchestration framework for hybrid microgrids combining battery storage, electrolyzers, and adaptive demand forecasting. GridSketch learns hierarchical control policies that coordinate solar, wind, and hydrogen subsystems using graph neural optimizers trained on dispatch logs from three island grids. Field trials in Barbados, Martinique, and Guadeloupe cut diesel peaker usage by 38% while maintaining reserve margins during cyclone-driven outages. Modular APIs let operators plug in market signals and carbon pricing objectives without rewriting controllers.\n",
    "\"\"\"\n",
    "\n",
    "result = analyzer(abstract=test_abstract)\n",
    "print(f\"Main Claim: {result.main_claim}\\n\")\n",
    "print(f\"Methods: {result.methods}\\n\")\n",
    "print(f\"Novelty Score: {result.novelty_score}/10\\n\")\n",
    "print(f\"Explanation: {result.novelty_explanation}\\n\")\n",
    "print(f\"Summary: {result.summary}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b368bf2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-21T13:06:34.673530Z",
     "iopub.status.busy": "2025-09-21T13:06:34.673530Z",
     "iopub.status.idle": "2025-09-21T13:06:37.695666Z",
     "shell.execute_reply": "2025-09-21T13:06:37.694389Z"
    },
    "id": "2b368bf2",
    "outputId": "b4600075-1302-49fd-9d07-72a5ad07cafe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 2 traces per predictor.\n",
      "Will attempt to bootstrap 5 candidate sets.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 1.00 / 1 (100.0%):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 2.00 / 2 (100.0%):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00, 11.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 3.00 / 3 (100.0%):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00, 13.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 3.00 / 3 (100.0%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 20.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 3.00 / 3 (100.0%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 20.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/21 06:06:34 INFO dspy.evaluate.evaluate: Average Metric: 3 / 3 (100.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 100.0 for seed -3\n",
      "Scores so far: [100.0]\n",
      "Best score so far: 100.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 1.00 / 1 (100.0%):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 2.00 / 2 (100.0%):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  6.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 2.00 / 2 (100.0%):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00, 12.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 3.00 / 3 (100.0%):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00, 12.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 3.00 / 3 (100.0%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 12.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/21 06:06:35 INFO dspy.evaluate.evaluate: Average Metric: 3 / 3 (100.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [100.0, 100.0]\n",
      "Best score so far: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  7.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  5.26it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  5.44it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 1.00 / 1 (100.0%):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 2.00 / 2 (100.0%):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 2.00 / 2 (100.0%):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  9.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 3.00 / 3 (100.0%):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  9.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 3.00 / 3 (100.0%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 3.00 / 3 (100.0%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/21 06:06:35 INFO dspy.evaluate.evaluate: Average Metric: 3 / 3 (100.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [100.0, 100.0, 100.0]\n",
      "Best score so far: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  7.94it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  7.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  7.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 1.00 / 1 (100.0%):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 2.00 / 2 (100.0%):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  9.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 2.00 / 2 (100.0%):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00, 18.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 3.00 / 3 (100.0%):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00, 18.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 3.00 / 3 (100.0%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 18.44it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/21 06:06:36 INFO dspy.evaluate.evaluate: Average Metric: 3 / 3 (100.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [100.0, 100.0, 100.0, 100.0]\n",
      "Best score so far: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  9.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  9.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 1.00 / 1 (100.0%):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 2.00 / 2 (100.0%):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  9.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 2.00 / 2 (100.0%):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00, 19.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 3.00 / 3 (100.0%):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00, 19.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 3.00 / 3 (100.0%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 19.95it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/21 06:06:36 INFO dspy.evaluate.evaluate: Average Metric: 3 / 3 (100.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [100.0, 100.0, 100.0, 100.0, 100.0]\n",
      "Best score so far: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  9.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  9.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 1.00 / 1 (100.0%):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 2.00 / 2 (100.0%):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00, 11.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 3.00 / 3 (100.0%):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00, 14.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 3.00 / 3 (100.0%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 21.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 3.00 / 3 (100.0%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 20.82it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/21 06:06:36 INFO dspy.evaluate.evaluate: Average Metric: 3 / 3 (100.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
      "Best score so far: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  8.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  7.95it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 1.00 / 1 (100.0%):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 2.00 / 2 (100.0%):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00, 10.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 3.00 / 3 (100.0%):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00, 14.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 3.00 / 3 (100.0%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 20.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 3.00 / 3 (100.0%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 20.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/21 06:06:37 INFO dspy.evaluate.evaluate: Average Metric: 3 / 3 (100.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
      "Best score so far: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  7.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  7.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 1.00 / 1 (100.0%):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 2.00 / 2 (100.0%):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  7.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 2.00 / 2 (100.0%):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00, 14.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 3.00 / 3 (100.0%):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00, 14.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Average Metric: 3.00 / 3 (100.0%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 16.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/21 06:06:37 INFO dspy.evaluate.evaluate: Average Metric: 3 / 3 (100.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
      "Best score so far: 100.0\n",
      "8 candidate programs found.\n",
      "Optimization complete! Let's test the improved analyzer:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimized Results:\n",
      "Main Claim: GridSketch is an orchestration framework for hybrid microgrids that reduces diesel peaker usage by 38% in island grids while maintaining reserve margins during cyclone-driven outages.\n",
      "Novelty Score: 7/10\n",
      "Summary: GridSketch is a novel orchestration framework for hybrid microgrids that effectively reduces diesel peaker usage by 38% in island grids, while ensuring reserve margins are maintained during cyclone-driven outages. The framework utilizes machine learning techniques, hierarchical control policies, and field trials in Barbados, Martinique, and Guadeloupe to achieve this significant reduction. Additionally, its modular APIs allow for easy integration of market signals and carbon pricing objectives without the need for rewriting controllers.\n"
     ]
    }
   ],
   "source": [
    "# The Power of Optimization: Bootstrap Few-Shot Learning\n",
    "def create_training_examples():\n",
    "    \"\"\"Create some training data for optimization\"\"\"\n",
    "    examples = []\n",
    "\n",
    "    examples.append(dspy.Example(\n",
    "        abstract=\"\"\"We introduce GridSketch, an orchestration layer for hybrid microgrids that fuses battery scheduling, electrolyzer control, and load forecasting through graph neural optimizers trained on island utility data.\"\"\",\n",
    "        main_claim=\"GridSketch coordinates hybrid microgrids using hierarchical graph control.\",\n",
    "        novelty_score=\"8\"\n",
    "    ).with_inputs('abstract'))\n",
    "\n",
    "    examples.append(dspy.Example(\n",
    "        abstract=\"\"\"HarborCast couples AIS shipping telemetry with ensemble weather forecasts to generate 48-hour energy demand scenarios for ports, guiding crane electrification and hydrogen storage commitments.\"\"\",\n",
    "        main_claim=\"HarborCast fuses shipping traffic and weather ensembles to forecast port energy demand.\",\n",
    "        novelty_score=\"7\"\n",
    "    ).with_inputs('abstract'))\n",
    "\n",
    "    examples.append(dspy.Example(\n",
    "        abstract=\"\"\"CryoSense deploys fiber Bragg grating arrays across fusion cryostats and learns thermal fingerprints that anticipate disruptive heat loads minutes before quench events.\"\"\",\n",
    "        main_claim=\"CryoSense maps fusion cryostat heat loads with fiber Bragg gratings to prevent quenches.\",\n",
    "        novelty_score=\"9\"\n",
    "    ).with_inputs('abstract'))\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "def validate_analysis(example, pred, trace=None):\n",
    "    \"\"\"Check if the analysis is reasonable\"\"\"\n",
    "    claim_valid = len(pred.main_claim) > 10 and len(pred.main_claim) < 200\n",
    "\n",
    "    try:\n",
    "        score = int(pred.novelty_score)\n",
    "        score_valid = 1 <= score <= 10\n",
    "    except Exception:\n",
    "        score_valid = False\n",
    "\n",
    "    summary_valid = len(pred.summary) > 50 and len(pred.summary) < 500\n",
    "\n",
    "    return claim_valid and score_valid and summary_valid\n",
    "\n",
    "\n",
    "trainset = create_training_examples()\n",
    "optimizer = BootstrapFewShotWithRandomSearch(\n",
    "    metric=validate_analysis,\n",
    "    max_bootstrapped_demos=2,\n",
    "    max_labeled_demos=2,\n",
    "    num_candidate_programs=5,\n",
    "    num_threads=1\n",
    ")\n",
    "\n",
    "compiled_analyzer = optimizer.compile(\n",
    "    ResearchPaperAnalyzer(),\n",
    "    trainset=trainset\n",
    ")\n",
    "\n",
    "print(\"Optimization complete! Let's test the improved analyzer:\")\n",
    "optimized = compiled_analyzer(abstract=test_abstract)\n",
    "print(f\"\\nOptimized Results:\")\n",
    "print(f\"Main Claim: {optimized.main_claim}\")\n",
    "print(f\"Novelty Score: {optimized.novelty_score}/10\")\n",
    "print(f\"Summary: {optimized.summary}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0955a8b7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-21T13:06:37.700833Z",
     "iopub.status.busy": "2025-09-21T13:06:37.699833Z",
     "iopub.status.idle": "2025-09-21T13:06:37.803846Z",
     "shell.execute_reply": "2025-09-21T13:06:37.802836Z"
    },
    "id": "0955a8b7",
    "outputId": "22ad428d-cf2a-4737-eaa3-343cb42d55d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning: To find the total volume of data archived each day after the changes, we need to follow these steps:\n",
      "1. Calculate the increase in volume after adding the hyperspectral sensor (35% of 120 GB).\n",
      "2. Determine the total volume after adding the hyperspectral sensor.\n",
      "3. Calculate the data removed after filtering (12% of the total volume).\n",
      "4. Subtract the data removed from the total volume to find the final amount of data archived each day.\n",
      "\n",
      "Answer: Let's calculate it step by step:\n",
      "1. Increase after adding hyperspectral sensor: 35% of 120 GB = 0.35 * 120 = 42 GB\n",
      "2. Total volume after adding sensor: 120 GB + 42 GB = 162 GB\n",
      "3. Data removed after filtering: 12% of 162 GB = 0.12 * 162 = 19.44 GB\n",
      "4. Final data archived each day: 162 GB - 19.44 GB = 142.56 GB\n"
     ]
    }
   ],
   "source": [
    "# Advanced Pattern: Chain of Thought Reasoning\n",
    "class ChainOfThoughtQA(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Use Chain of Thought for complex reasoning\n",
    "        self.generate_answer = dspy.ChainOfThought(\"question -> reasoning, answer\")\n",
    "\n",
    "    def forward(self, question):\n",
    "        result = self.generate_answer(question=question)\n",
    "        return result\n",
    "\n",
    "\n",
    "cot_qa = ChainOfThoughtQA()\n",
    "complex_question = \"\"\"\n",
    "A climate observatory stores 120 GB of raw satellite data per day. Adding a hyperspectral sensor increases the volume by 35%, and automated filtering removes 12% of the combined data. How many gigabytes are archived each day after these changes?\n",
    "\"\"\"\n",
    "\n",
    "cot_result = cot_qa(question=complex_question)\n",
    "print(f\"Reasoning: {cot_result.reasoning}\\n\")\n",
    "print(f\"Answer: {cot_result.answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "871903d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-21T13:06:37.813149Z",
     "iopub.status.busy": "2025-09-21T13:06:37.812155Z",
     "iopub.status.idle": "2025-09-21T13:06:37.826550Z",
     "shell.execute_reply": "2025-09-21T13:06:37.824339Z"
    },
    "id": "871903d1",
    "outputId": "4b68cb03-9ece-4353-d0c0-37a013240432"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MultiHopQA module skeleton ready. Configure a retriever before calling it.\n"
     ]
    }
   ],
   "source": [
    "# Building a Multi-Hop Question Answering System\n",
    "class MultiHopQA(dspy.Module):\n",
    "    def __init__(self, passages_per_hop=3):\n",
    "        super().__init__()\n",
    "        self.passages_per_hop = passages_per_hop\n",
    "\n",
    "        self.generate_query = dspy.ChainOfThought(\"context, question -> reasoning, query\")\n",
    "        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n",
    "        self.generate_answer = dspy.ChainOfThought(\"context, question -> reasoning, answer\")\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = []\n",
    "        for hop in range(2):\n",
    "            query_result = self.generate_query(\n",
    "                context=\"\\n\".join(context) if context else \"No context yet\",\n",
    "                question=question\n",
    "            )\n",
    "            passages = self.retrieve(query_result.query).passages\n",
    "            context.extend(passages[:2])\n",
    "\n",
    "        final_answer = self.generate_answer(\n",
    "            context=\"\\n\".join(context),\n",
    "            question=question\n",
    "        )\n",
    "\n",
    "        return dspy.Prediction(\n",
    "            answer=final_answer.answer,\n",
    "            reasoning=final_answer.reasoning,\n",
    "            supporting_passages=context\n",
    "        )\n",
    "\n",
    "print(\"âœ… MultiHopQA module skeleton ready. Configure a retriever before calling it.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b724e56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-21T13:06:37.834825Z",
     "iopub.status.busy": "2025-09-21T13:06:37.833822Z",
     "iopub.status.idle": "2025-09-21T13:06:37.995151Z",
     "shell.execute_reply": "2025-09-21T13:06:37.992340Z"
    },
    "id": "2b724e56",
    "outputId": "1b6893f2-f162-4c65-e7eb-3c4748096282"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: aggregate_power_windows\n",
      "\n",
      "Generated Documentation:\n",
      "\"\"\"\n",
      "\"\"\"\n",
      "Collapse high-frequency sensor readings into rolling demand windows.\n",
      "\n",
      "Args:\n",
      "    readings (list[dict]): A list of dictionaries containing sensor readings.\n",
      "    window_minutes (int, optional): The size of the rolling demand windows in minutes. Defaults to 15.\n",
      "\n",
      "Returns:\n",
      "    dict[str, float]: A dictionary where keys are meter IDs and values are the average kilowatt readings within the window.\n",
      "\"\"\"\n",
      "\n",
      "Example:\n",
      "    >>> # Example of using aggregate_power_windows function\n",
      "readings = [\n",
      "    {\"meter_id\": \"A\", \"kilowatt\": 10.5},\n",
      "    {\"meter_id\": \"B\", \"kilowatt\": 20.3},\n",
      "    {\"meter_id\": \"A\", \"kilowatt\": 9.2},\n",
      "    {\"meter_id\": \"B\", \"kilowatt\": 21.1},\n",
      "    {\"meter_id\": \"A\", \"kilowatt\": 11.7}\n",
      "]\n",
      "\n",
      "window_minutes = 10\n",
      "result = aggregate_power_windows(readings, window_minutes)\n",
      "print(result)\n",
      "# Output: {'A': 9.85, 'B': 20.7}\n",
      "\"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Practical Example: Building a Code Documentation Generator\n",
    "class ExtractFunctionInfo(dspy.Signature):\n",
    "    \"\"\"Extract key information from Python function code.\"\"\"\n",
    "\n",
    "    code = dspy.InputField(desc=\"Python function code\")\n",
    "    function_name = dspy.OutputField()\n",
    "    parameters = dspy.OutputField(desc=\"list of parameters with types if available\")\n",
    "    return_type = dspy.OutputField(desc=\"return type if specified, otherwise 'inferred'\")\n",
    "\n",
    "\n",
    "class GenerateDocstring(dspy.Signature):\n",
    "    \"\"\"Generate a comprehensive docstring for a Python function.\"\"\"\n",
    "\n",
    "    code = dspy.InputField()\n",
    "    function_name = dspy.InputField()\n",
    "    parameters = dspy.InputField()\n",
    "    return_type = dspy.InputField()\n",
    "    docstring = dspy.OutputField(desc=\"Google-style docstring without the triple quotes\")\n",
    "\n",
    "\n",
    "class GenerateUsageExample(dspy.Signature):\n",
    "    \"\"\"Generate a usage example for the function.\"\"\"\n",
    "\n",
    "    function_name = dspy.InputField()\n",
    "    parameters = dspy.InputField()\n",
    "    docstring = dspy.InputField()\n",
    "    example_code = dspy.OutputField(desc=\"simple example showing function usage\")\n",
    "\n",
    "\n",
    "class CodeDocumentationGenerator(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.extract_info = dspy.Predict(ExtractFunctionInfo)\n",
    "        self.generate_docstring = dspy.Predict(GenerateDocstring)\n",
    "        self.generate_example = dspy.Predict(GenerateUsageExample)\n",
    "\n",
    "    def forward(self, code):\n",
    "        info = self.extract_info(code=code)\n",
    "\n",
    "        docstring = self.generate_docstring(\n",
    "            code=code,\n",
    "            function_name=info.function_name,\n",
    "            parameters=info.parameters,\n",
    "            return_type=info.return_type\n",
    "        ).docstring\n",
    "\n",
    "        example = self.generate_example(\n",
    "            function_name=info.function_name,\n",
    "            parameters=info.parameters,\n",
    "            docstring=docstring\n",
    "        ).example_code\n",
    "\n",
    "        documentation = (\n",
    "            '\"\"\"\\n'\n",
    "            + docstring\n",
    "            + '\\n\\nExample:\\n    >>> '\n",
    "            + example\n",
    "            + '\\n\"\"\"'\n",
    "        )\n",
    "\n",
    "        return dspy.Prediction(\n",
    "            function_name=info.function_name,\n",
    "            documentation=documentation,\n",
    "            docstring=docstring,\n",
    "            example=example\n",
    "        )\n",
    "\n",
    "\n",
    "doc_generator = CodeDocumentationGenerator()\n",
    "test_code = '''\n",
    "import collections\n",
    "\n",
    "def aggregate_power_windows(readings: list[dict], window_minutes: int = 15) -> dict[str, float]:\n",
    "    \"\"\"Collapse high-frequency sensor readings into rolling demand windows.\"\"\"\n",
    "    buckets: dict[str, list[float]] = collections.defaultdict(list)\n",
    "    for entry in readings:\n",
    "        buckets[entry[\"meter_id\"]].append(entry[\"kilowatts\"])\n",
    "    return {\n",
    "        meter: sum(values) / len(values)\n",
    "        for meter, values in buckets.items()\n",
    "        if values\n",
    "    }\n",
    "'''\n",
    "\n",
    "documentation_result = doc_generator(code=test_code)\n",
    "print(f\"Function: {documentation_result.function_name}\\n\")\n",
    "print(\"Generated Documentation:\")\n",
    "print(documentation_result.documentation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4266ead",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "execution": {
     "iopub.execute_input": "2025-09-21T13:06:38.001151Z",
     "iopub.status.busy": "2025-09-21T13:06:38.000152Z",
     "iopub.status.idle": "2025-09-21T13:06:38.343576Z",
     "shell.execute_reply": "2025-09-21T13:06:38.342569Z"
    },
    "id": "e4266ead",
    "outputId": "fa11aa05-acd5-4b87-b2c1-4233fa011580"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/21 06:06:38 INFO dspy.evaluate.evaluate: Average Metric: 3 / 3 (100.0%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>example_function_name</th>\n",
       "      <th>pred_function_name</th>\n",
       "      <th>documentation</th>\n",
       "      <th>docstring</th>\n",
       "      <th>example</th>\n",
       "      <th>documentation_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def compute_capacity_factor(energy_mwh: float, rated_power_mw: flo...</td>\n",
       "      <td>compute_capacity_factor</td>\n",
       "      <td>compute_capacity_factor</td>\n",
       "      <td>\"\"\" Calculates the capacity factor of a power plant based on the e...</td>\n",
       "      <td>Calculates the capacity factor of a power plant based on the energ...</td>\n",
       "      <td># Example usage of the compute_capacity_factor function\\nenergy_mw...</td>\n",
       "      <td>âœ”ï¸ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def rolling_mean(series: list[float], window: int) -&gt; list[float]:...</td>\n",
       "      <td>rolling_mean</td>\n",
       "      <td>rolling_mean</td>\n",
       "      <td>\"\"\" Calculates the rolling mean of a series with a specified windo...</td>\n",
       "      <td>Calculates the rolling mean of a series with a specified window si...</td>\n",
       "      <td># Example of using rolling_mean function\\nseries = [1.2, 2.3, 3.4,...</td>\n",
       "      <td>âœ”ï¸ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def fuse_weather_signals(temperature: float, wind_speed: float, hu...</td>\n",
       "      <td>fuse_weather_signals</td>\n",
       "      <td>fuse_weather_signals</td>\n",
       "      <td>\"\"\" Calculates a fused weather signal based on temperature, wind s...</td>\n",
       "      <td>Calculates a fused weather signal based on temperature, wind speed...</td>\n",
       "      <td># Example usage of fuse_weather_signals function\\nfused_signal = f...</td>\n",
       "      <td>âœ”ï¸ [True]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    code  \\\n",
       "0  def compute_capacity_factor(energy_mwh: float, rated_power_mw: flo...   \n",
       "1  def rolling_mean(series: list[float], window: int) -> list[float]:...   \n",
       "2  def fuse_weather_signals(temperature: float, wind_speed: float, hu...   \n",
       "\n",
       "     example_function_name       pred_function_name  \\\n",
       "0  compute_capacity_factor  compute_capacity_factor   \n",
       "1             rolling_mean             rolling_mean   \n",
       "2     fuse_weather_signals     fuse_weather_signals   \n",
       "\n",
       "                                                           documentation  \\\n",
       "0  \"\"\" Calculates the capacity factor of a power plant based on the e...   \n",
       "1  \"\"\" Calculates the rolling mean of a series with a specified windo...   \n",
       "2  \"\"\" Calculates a fused weather signal based on temperature, wind s...   \n",
       "\n",
       "                                                               docstring  \\\n",
       "0  Calculates the capacity factor of a power plant based on the energ...   \n",
       "1  Calculates the rolling mean of a series with a specified window si...   \n",
       "2  Calculates a fused weather signal based on temperature, wind speed...   \n",
       "\n",
       "                                                                 example  \\\n",
       "0  # Example usage of the compute_capacity_factor function\\nenergy_mw...   \n",
       "1  # Example of using rolling_mean function\\nseries = [1.2, 2.3, 3.4,...   \n",
       "2  # Example usage of fuse_weather_signals function\\nfused_signal = f...   \n",
       "\n",
       "  documentation_metric  \n",
       "0            âœ”ï¸ [True]  \n",
       "1            âœ”ï¸ [True]  \n",
       "2            âœ”ï¸ [True]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Score: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation and Testing\n",
    "def create_test_set():\n",
    "    \"\"\"Create test examples for our documentation generator\"\"\"\n",
    "    test_examples = []\n",
    "\n",
    "    test_examples.append(dspy.Example(\n",
    "        code='''def compute_capacity_factor(energy_mwh: float, rated_power_mw: float) -> float:\\n    return energy_mwh / (rated_power_mw * 8760)''',\n",
    "        function_name=\"compute_capacity_factor\"\n",
    "    ).with_inputs('code'))\n",
    "\n",
    "    test_examples.append(dspy.Example(\n",
    "        code='''def rolling_mean(series: list[float], window: int) -> list[float]:\\n    return [sum(series[i:i+window]) / window for i in range(len(series) - window + 1)]''',\n",
    "        function_name=\"rolling_mean\"\n",
    "    ).with_inputs('code'))\n",
    "\n",
    "    test_examples.append(dspy.Example(\n",
    "        code='''def fuse_weather_signals(temperature: float, wind_speed: float, humidity: float) -> float:\\n    return 0.4 * temperature + 0.35 * wind_speed + 0.25 * humidity''',\n",
    "        function_name=\"fuse_weather_signals\"\n",
    "    ).with_inputs('code'))\n",
    "\n",
    "    return test_examples\n",
    "\n",
    "\n",
    "def documentation_metric(example, pred, trace=None):\n",
    "    \"\"\"Evaluate documentation quality\"\"\"\n",
    "    name_correct = pred.function_name == example.function_name\n",
    "    doc_exists = len(pred.documentation) > 50\n",
    "    example_exists = len(pred.example) > 10\n",
    "    return name_correct and doc_exists and example_exists\n",
    "\n",
    "\n",
    "evaluator = Evaluate(\n",
    "    devset=create_test_set(),\n",
    "    num_threads=1,\n",
    "    display_progress=False,\n",
    "    display_table=5\n",
    ")\n",
    "\n",
    "raw_score = evaluator(doc_generator, metric=documentation_metric)\n",
    "score_value = getattr(raw_score, \"score\", raw_score)\n",
    "score_fraction = score_value / 100 if score_value > 1 else score_value\n",
    "print(f\"\\nEvaluation Score: {score_fraction:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08b3e810",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-21T13:06:38.346579Z",
     "iopub.status.busy": "2025-09-21T13:06:38.346579Z",
     "iopub.status.idle": "2025-09-21T13:06:38.403406Z",
     "shell.execute_reply": "2025-09-21T13:06:38.402395Z"
    },
    "id": "08b3e810",
    "outputId": "1e608a75-1110-4b66-f744-061d184c8b37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation failed: module 'dspy' has no attribute 'Suggest'\n"
     ]
    }
   ],
   "source": [
    "class ValidatedQA(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_answer = dspy.ChainOfThought(\"question -> answer\")\n",
    "\n",
    "    def forward(self, question):\n",
    "        pred = self.generate_answer(question=question)\n",
    "\n",
    "        dspy.Suggest(\n",
    "            len(pred.answer) > 10,\n",
    "            \"Answer should be at least 10 characters long\"\n",
    "        )\n",
    "\n",
    "        dspy.Suggest(\n",
    "            not pred.answer.lower().startswith(\"i don't know\"),\n",
    "            \"Answer should be informative\"\n",
    "        )\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "validated_qa = ValidatedQA()\n",
    "try:\n",
    "    answer = validated_qa(question=\"What is the primary purpose of a synthetic aperture radar constellation in climate monitoring?\")\n",
    "    print(f\"Answer: {answer.answer}\")\n",
    "except Exception as exc:\n",
    "    print(f\"Validation failed: {exc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "366a864b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-21T13:06:38.408684Z",
     "iopub.status.busy": "2025-09-21T13:06:38.408684Z",
     "iopub.status.idle": "2025-09-21T13:06:39.933224Z",
     "shell.execute_reply": "2025-09-21T13:06:39.932714Z"
    },
    "id": "366a864b",
    "outputId": "d356f856-37fa-49de-dd07-20a9c109acd7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  9.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  9.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  9.32it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  9.26it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 2 examples for up to 1 rounds, amounting to 3 attempts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom optimizer selected: ResearchPaperAnalyzer\n"
     ]
    }
   ],
   "source": [
    "# Custom Teleprompters for Optimization\n",
    "class CustomOptimizer:\n",
    "    def __init__(self, metric, examples):\n",
    "        self.metric = metric\n",
    "        self.examples = examples\n",
    "\n",
    "    def compile(self, module):\n",
    "        strategies = []\n",
    "\n",
    "        if len(self.examples) > 0:\n",
    "            labeled = LabeledFewShot(k=min(3, len(self.examples)))\n",
    "            strategies.append(labeled.compile(module, trainset=self.examples))\n",
    "\n",
    "        bootstrap = BootstrapFewShot(\n",
    "            metric=self.metric,\n",
    "            max_bootstrapped_demos=4\n",
    "        )\n",
    "        strategies.append(bootstrap.compile(module, trainset=self.examples))\n",
    "\n",
    "        best_score = 0\n",
    "        best_module = module\n",
    "\n",
    "        for strategy in strategies:\n",
    "            score = self.evaluate_module(strategy)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_module = strategy\n",
    "\n",
    "        return best_module\n",
    "\n",
    "    def evaluate_module(self, module):\n",
    "        correct = 0\n",
    "        for example in self.examples[:3]:\n",
    "            try:\n",
    "                pred = module(example.inputs())\n",
    "                if self.metric(example, pred):\n",
    "                    correct += 1\n",
    "            except Exception:\n",
    "                pass\n",
    "        denominator = min(3, len(self.examples)) or 1\n",
    "        return correct / denominator\n",
    "\n",
    "\n",
    "custom_optimizer = CustomOptimizer(metric=validate_analysis, examples=create_training_examples())\n",
    "optimized_module = custom_optimizer.compile(ResearchPaperAnalyzer())\n",
    "print(\"Custom optimizer selected:\", type(optimized_module).__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41ea6608",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-21T13:06:39.938236Z",
     "iopub.status.busy": "2025-09-21T13:06:39.938236Z",
     "iopub.status.idle": "2025-09-21T13:06:40.028620Z",
     "shell.execute_reply": "2025-09-21T13:06:40.019933Z"
    },
    "id": "41ea6608",
    "outputId": "c6350b95-9517-401c-fd80-54f88b5a40b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Which country operates the Atacama Large Millimeter/submillimeter Array?\n",
      "A: Chile\n",
      "\n",
      "Q: Where is the headquarters of the International Renewable Energy Agency located?\n",
      "A: Abu Dhabi, United Arab Emirates\n",
      "\n",
      "Q: Which city hosts Europe's largest battery gigafactory Northvolt Ett?\n",
      "A: SkellefteÃ¥\n",
      "\n",
      "Q: Which country launched the Himawari-9 weather satellite?\n",
      "A: Japan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parallel Processing for Batch Operations\n",
    "class BatchProcessor(dspy.Module):\n",
    "    def __init__(self, base_module, max_workers=4):\n",
    "        super().__init__()\n",
    "        self.base_module = base_module\n",
    "        self.max_workers = max_workers\n",
    "\n",
    "    def forward(self, items):\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            futures = [executor.submit(self.base_module, item) for item in items]\n",
    "            results = [future.result() for future in futures]\n",
    "        return results\n",
    "\n",
    "\n",
    "qa_module = SimpleQAModule()\n",
    "batch_processor = BatchProcessor(qa_module)\n",
    "\n",
    "questions = [\n",
    "    \"Which country operates the Atacama Large Millimeter/submillimeter Array?\",\n",
    "    \"Where is the headquarters of the International Renewable Energy Agency located?\",\n",
    "    \"Which city hosts Europe's largest battery gigafactory Northvolt Ett?\",\n",
    "    \"Which country launched the Himawari-9 weather satellite?\"\n",
    "]\n",
    "\n",
    "batch_results = batch_processor(questions)\n",
    "for q, r in zip(questions, batch_results):\n",
    "    print(f\"Q: {q}\\nA: {r}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfae0b6f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-21T13:06:40.060450Z",
     "iopub.status.busy": "2025-09-21T13:06:40.060450Z",
     "iopub.status.idle": "2025-09-21T13:06:41.429331Z",
     "shell.execute_reply": "2025-09-21T13:06:41.428314Z"
    },
    "id": "dfae0b6f",
    "outputId": "9a29460d-5f47-48ae-f5ef-590b88686f46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FastAPI app configured. Run uvicorn main:app --reload in a terminal to try it out.\n"
     ]
    }
   ],
   "source": [
    "# Real-World Integration: Building an API\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "doc_generator_api = CodeDocumentationGenerator()\n",
    "\n",
    "\n",
    "class CodeRequest(BaseModel):\n",
    "    code: str\n",
    "\n",
    "\n",
    "class DocumentationResponse(BaseModel):\n",
    "    function_name: str\n",
    "    documentation: str\n",
    "    example: str\n",
    "\n",
    "\n",
    "@app.post(\"/generate-docs\", response_model=DocumentationResponse)\n",
    "async def generate_documentation(request: CodeRequest):\n",
    "    try:\n",
    "        result = doc_generator_api(code=request.code)\n",
    "        return DocumentationResponse(\n",
    "            function_name=result.function_name,\n",
    "            documentation=result.documentation,\n",
    "            example=result.example\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        raise HTTPException(status_code=500, detail=str(exc))\n",
    "\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\"status\": \"healthy\", \"model\": \"DSPy Documentation Generator\"}\n",
    "\n",
    "print(\"âœ… FastAPI app configured. Run uvicorn main:app --reload in a terminal to try it out.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bca7094",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-21T13:06:41.438328Z",
     "iopub.status.busy": "2025-09-21T13:06:41.437335Z",
     "iopub.status.idle": "2025-09-21T13:06:41.576841Z",
     "shell.execute_reply": "2025-09-21T13:06:41.569312Z"
    },
    "id": "8bca7094",
    "outputId": "6f119cee-d318-4e6b-9e81-c1a6f210eeb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Saved and reloaded the compiled analyzer from disk.\n"
     ]
    }
   ],
   "source": [
    "# Performance Optimization Tips\n",
    "def save_compiled_module(module, filename):\n",
    "    \"\"\"Save a compiled DSPy module to disk\"\"\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(module, f)\n",
    "\n",
    "\n",
    "def load_compiled_module(filename):\n",
    "    \"\"\"Load a pre-compiled DSPy module\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "class CachedModule(dspy.Module):\n",
    "    def __init__(self, base_module):\n",
    "        super().__init__()\n",
    "        self.base_module = base_module\n",
    "        self.cache = {}\n",
    "\n",
    "    def _hash_input(self, input_data):\n",
    "        \"\"\"Create a hash of the input for caching\"\"\"\n",
    "        return hashlib.md5(str(input_data).encode()).hexdigest()\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        cache_key = self._hash_input(kwargs)\n",
    "        if cache_key not in self.cache:\n",
    "            self.cache[cache_key] = self.base_module(**kwargs)\n",
    "        return self.cache[cache_key]\n",
    "\n",
    "\n",
    "def process_in_batches(module, items, batch_size=10):\n",
    "    \"\"\"Process items in batches to optimize LLM calls\"\"\"\n",
    "    results = []\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        batch = items[i:i + batch_size]\n",
    "        batch_results = [module(item) for item in batch]\n",
    "        results.extend(batch_results)\n",
    "    return results\n",
    "\n",
    "\n",
    "if 'compiled_analyzer' in globals():\n",
    "    save_compiled_module(compiled_analyzer, 'compiled_analyzer.pkl')\n",
    "    restored = load_compiled_module('compiled_analyzer.pkl')\n",
    "    print(\"ðŸ“¦ Saved and reloaded the compiled analyzer from disk.\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ Compile a module first, then call save_compiled_module to persist it.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f44bd150",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-21T13:06:41.590617Z",
     "iopub.status.busy": "2025-09-21T13:06:41.588616Z",
     "iopub.status.idle": "2025-09-21T13:06:41.771037Z",
     "shell.execute_reply": "2025-09-21T13:06:41.768520Z"
    },
    "id": "f44bd150",
    "outputId": "f0cf0142-abe6-4b13-c281-a09ef6abfe65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Question: What is a geospatial digital twin and how is it used in climate resilience planning?\n",
      "\n",
      "Note: Cannot directly print prompt using .get_prompt() in this DSPy version.\n",
      "\n",
      "Result: A geospatial digital twin is a digital representation of a physical asset, process, or system that includes geospatial data. It is used in climate resilience planning to simulate and analyze potential impacts of climate change on infrastructure, natural resources, and communities. By incorporating geospatial data into the digital twin, planners can better understand vulnerabilities, identify adaptation strategies, and optimize resilience measures to mitigate the effects of climate change.\n",
      "\n",
      "Trace events collected:\n",
      "- (Predict(BasicQA(question -> answer\n",
      "    instructions='Answer questions with short factual answers.'\n",
      "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': 'often between 1 and 5 words', '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
      ")), {'question': 'Which country operates the Atacama Large Millimeter/submillimeter Array?'}, Prediction(\n",
      "    answer='Chile'\n",
      "))\n",
      "- (Predict(BasicQA(question -> answer\n",
      "    instructions='Answer questions with short factual answers.'\n",
      "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': 'often between 1 and 5 words', '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
      ")), {'question': \"Which city hosts Europe's largest battery gigafactory Northvolt Ett?\"}, Prediction(\n",
      "    answer='SkellefteÃ¥'\n",
      "))\n",
      "- (Predict(BasicQA(question -> answer\n",
      "    instructions='Answer questions with short factual answers.'\n",
      "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': 'often between 1 and 5 words', '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
      ")), {'question': 'Where is the headquarters of the International Renewable Energy Agency located?'}, Prediction(\n",
      "    answer='Abu Dhabi, United Arab Emirates'\n",
      "))\n",
      "- (Predict(BasicQA(question -> answer\n",
      "    instructions='Answer questions with short factual answers.'\n",
      "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': 'often between 1 and 5 words', '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
      ")), {'question': 'Which country launched the Himawari-9 weather satellite?'}, Prediction(\n",
      "    answer='Japan'\n",
      "))\n",
      "- (Predict(StringSignature(question -> answer\n",
      "    instructions='Given the fields `question`, produce the fields `answer`.'\n",
      "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Answer:', 'desc': '${answer}'})\n",
      ")), {'question': 'What is a geospatial digital twin and how is it used in climate resilience planning?'}, Prediction(\n",
      "    answer='A geospatial digital twin is a digital representation of a physical asset, process, or system that includes geospatial data. It is used in climate resilience planning to simulate and analyze potential impacts of climate change on infrastructure, natural resources, and communities. By incorporating geospatial data into the digital twin, planners can better understand vulnerabilities, identify adaptation strategies, and optimize resilience measures to mitigate the effects of climate change.'\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "# Debugging DSPy Programs\n",
    "dspy.settings.configure(lm=dspy.settings.lm, trace=TRACE_LOG)\n",
    "\n",
    "class DebugModule(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.predict = dspy.Predict(\"question -> answer\")\n",
    "\n",
    "    def forward(self, question):\n",
    "        print(f\"Input Question: {question}\")\n",
    "        result = self.predict(question=question)\n",
    "        print(\"\\nNote: Cannot directly print prompt using .get_prompt() in this DSPy version.\")\n",
    "        print(f\"\\nResult: {result.answer}\")\n",
    "        return result\n",
    "\n",
    "\n",
    "debug_module = DebugModule()\n",
    "debug_result = debug_module(question=\"What is a geospatial digital twin and how is it used in climate resilience planning?\")\n",
    "\n",
    "print(\"\\nTrace events collected:\")\n",
    "if TRACE_LOG:\n",
    "    for item in TRACE_LOG[-5:]:\n",
    "        print(f\"- {item}\")\n",
    "else:\n",
    "    print(\"(Trace log is empty in this simulated run.)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96ab0530",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-21T13:06:41.779573Z",
     "iopub.status.busy": "2025-09-21T13:06:41.779573Z",
     "iopub.status.idle": "2025-09-21T13:06:41.797911Z",
     "shell.execute_reply": "2025-09-21T13:06:41.797402Z"
    },
    "id": "96ab0530",
    "outputId": "83bd1331-27c8-4fd5-8f6e-29ad1cfbb7dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple predictor ready: Predict(StringSignature(question -> answer\n",
      "    instructions='Given the fields `question`, produce the fields `answer`.'\n",
      "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Answer:', 'desc': '${answer}'})\n",
      "))\n",
      "Chain-of-thought predictor ready: predict = Predict(StringSignature(context, question -> reasoning, answer\n",
      "    instructions='Given the fields `context`, `question`, produce the fields `reasoning`, `answer`.'\n",
      "    context = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Context:', 'desc': '${context}'})\n",
      "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
      "    reasoning = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Reasoning:', 'desc': '${reasoning}'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Answer:', 'desc': '${answer}'})\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "# Lessons Learned: Start Simple, Then Optimize\n",
    "simple = dspy.Predict(\"question -> answer\")\n",
    "complex = dspy.ChainOfThought(\"context, question -> reasoning, answer\")\n",
    "print(\"Simple predictor ready:\", simple)\n",
    "print(\"Chain-of-thought predictor ready:\", complex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4403019",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-21T13:06:41.805378Z",
     "iopub.status.busy": "2025-09-21T13:06:41.805378Z",
     "iopub.status.idle": "2025-09-21T13:06:41.816762Z",
     "shell.execute_reply": "2025-09-21T13:06:41.815746Z"
    },
    "id": "c4403019",
    "outputId": "b0128dbd-da8b-441f-9be9-921ca09db66d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example({'question': 'Which ports are most exposed to extreme heat disruptions by 2030?'}) (input_keys={'question'})\n"
     ]
    }
   ],
   "source": [
    "# Lessons Learned: Invest in Good Training Data\n",
    "def create_high_quality_example(input_data, expected_output):\n",
    "    \"\"\"Create well-structured training examples\"\"\"\n",
    "    example = dspy.Example(\n",
    "        **input_data,\n",
    "        **expected_output\n",
    "    ).with_inputs(*input_data.keys())\n",
    "\n",
    "    assert all(v is not None for v in input_data.values())\n",
    "    assert all(v is not None for v in expected_output.values())\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "example = create_high_quality_example(\n",
    "    {\"question\": \"Which ports are most exposed to extreme heat disruptions by 2030?\"},\n",
    "    {\"answer\": \"Ports in the Arabian Gulf and Southeast Asia face the highest combined heat risk.\"}\n",
    ")\n",
    "print(example.inputs())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1411885e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-21T13:06:41.824257Z",
     "iopub.status.busy": "2025-09-21T13:06:41.824257Z",
     "iopub.status.idle": "2025-09-21T13:06:41.832010Z",
     "shell.execute_reply": "2025-09-21T13:06:41.831002Z"
    },
    "id": "1411885e",
    "outputId": "49d3193f-c87c-421d-f676-5b2b765e7b29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ³ï¸ AssertiveModule defined as a template. Plug in your own process() implementation before calling it.\n"
     ]
    }
   ],
   "source": [
    "# Lessons Learned: Use Assertions Wisely\n",
    "class AssertiveModule(dspy.Module):\n",
    "    def forward(self, input_text):\n",
    "        result = self.process(input_text)\n",
    "\n",
    "        dspy.Suggest(len(result) > 10, \"Output too short\")\n",
    "        dspy.Assert(result is not None, \"Output cannot be None\")\n",
    "\n",
    "        return result\n",
    "\n",
    "print(\"âœ³ï¸ AssertiveModule defined as a template. Plug in your own process() implementation before calling it.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e39b9988",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-21T13:06:41.838028Z",
     "iopub.status.busy": "2025-09-21T13:06:41.837372Z",
     "iopub.status.idle": "2025-09-21T13:06:41.874271Z",
     "shell.execute_reply": "2025-09-21T13:06:41.872228Z"
    },
    "id": "e39b9988",
    "outputId": "de10ff1a-34a7-448d-b1e8-52d2ca3cd354"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitored output: Synthesised payload: {'payload': 'Monitor turbine cluster'}\n",
      "Metrics summary: Success Rate: 100.00%, Avg Time: 0.00s\n"
     ]
    }
   ],
   "source": [
    "# Lessons Learned: Monitor and Log Performance\n",
    "class MonitoredModule(dspy.Module):\n",
    "    def __init__(self, base_module):\n",
    "        super().__init__()\n",
    "        self.base_module = base_module\n",
    "        self.metrics = []\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        try:\n",
    "            result = self.base_module(**kwargs)\n",
    "            success = True\n",
    "            error = None\n",
    "        except Exception as exc:\n",
    "            success = False\n",
    "            error = str(exc)\n",
    "            result = None\n",
    "\n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "        self.metrics.append({\n",
    "            'timestamp': datetime.now(),\n",
    "            'success': success,\n",
    "            'elapsed_time': elapsed,\n",
    "            'error': error\n",
    "        })\n",
    "\n",
    "        if not success:\n",
    "            raise Exception(error)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_metrics_summary(self):\n",
    "        if not self.metrics:\n",
    "            return \"No metrics collected\"\n",
    "\n",
    "        success_rate = sum(m['success'] for m in self.metrics) / len(self.metrics)\n",
    "        avg_time = sum(m['elapsed_time'] for m in self.metrics) / len(self.metrics)\n",
    "\n",
    "        return f\"Success Rate: {success_rate:.2%}, Avg Time: {avg_time:.2f}s\"\n",
    "\n",
    "\n",
    "monitored = MonitoredModule(lambda **kwargs: f\"Synthesised payload: {kwargs}\")\n",
    "demo_output = monitored(payload=\"Monitor turbine cluster\")\n",
    "print(\"Monitored output:\", demo_output)\n",
    "print(\"Metrics summary:\", monitored.get_metrics_summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c233a6",
   "metadata": {},
   "source": [
    "## Fine-Tuning and Distillation\n",
    "\n",
    "The last step in the workflow is to distil richer reasoning traces into a lighter module. We start by letting the chain-of-thought teacher answer a set of energy-transition questions, then compile a prompt-optimised student. Finally, we show how a weight-level fine-tune would be wired up when a trainable local LM is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "add6775a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:06:41.882860Z",
     "iopub.status.busy": "2025-09-21T13:06:41.882860Z",
     "iopub.status.idle": "2025-09-21T13:06:44.507484Z",
     "shell.execute_reply": "2025-09-21T13:06:44.507484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 48 distillation pairs across 12 canonical questions.\n",
      "Sample teacher trace:\n",
      "- Q: Which country hosts the ITER fusion reactor construction site?\n",
      "  Teacher A: France\n",
      "  Reasoning: The ITER fusion reactor construction site is located in the southern region of France.\n",
      "- Q: In which European country is the ITER tokamak being assembled?\n",
      "  Teacher A: France\n",
      "  Reasoning: The ITER tokamak is being assembled in France. The project is a collaboration between 35 countries, including the European Union, China, India, Japan, Russia, South Korea, and the United States. The construction site is located in Cadarache, a commune in the Provence-Alpes-CÃ´te d'Azur region in southeastern France.\n",
      "- Q: Which nation hosts the construction of the ITER fusion reactor?\n",
      "  Teacher A: France\n",
      "  Reasoning: The ITER fusion reactor is being constructed in France.\n",
      "- Q: Where is the ITER fusion megaproject located?\n",
      "  Teacher A: Southern France\n",
      "  Reasoning: The ITER fusion megaproject is located in southern France.\n",
      "- Q: What is the capital of the country that operates the ALMA observatory?\n",
      "  Teacher A: Santiago\n",
      "  Reasoning: The ALMA (Atacama Large Millimeter/submillimeter Array) observatory is located in Chile. Therefore, the capital of the country that operates the ALMA observatory is Santiago.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "QUESTION_BANK = [\n",
    "    {\n",
    "        \"canonical_question\": \"Which country hosts the ITER fusion reactor construction site?\",\n",
    "        \"answer\": \"France\",\n",
    "        \"aliases\": [\"France\", \"Southern France\", \"South of France\"],\n",
    "        \"paraphrases\": [\n",
    "            \"In which European country is the ITER tokamak being assembled?\",\n",
    "            \"Which nation hosts the construction of the ITER fusion reactor?\",\n",
    "            \"Where is the ITER fusion megaproject located?\"\n",
    "        ],\n",
    "        \"context\": \"The ITER experimental fusion reactor is being constructed in southern France near the Cadarache research center.\"\n",
    "    },\n",
    "    {\n",
    "        \"canonical_question\": \"What is the capital of the country that operates the ALMA observatory?\",\n",
    "        \"answer\": \"Santiago\",\n",
    "        \"aliases\": [\"Santiago\", \"Santiago de Chile\"],\n",
    "        \"paraphrases\": [\n",
    "            \"Name the capital city of the nation that operates the ALMA observatory.\",\n",
    "            \"What city is the capital of Chile, the country running ALMA?\",\n",
    "            \"Which city is the capital of the country that owns the ALMA array?\"\n",
    "        ],\n",
    "        \"context\": \"ALMA is operated by Chile; Chile's capital city is Santiago.\"\n",
    "    },\n",
    "    {\n",
    "        \"canonical_question\": \"Which nation leads the world's installed offshore wind capacity in 2024?\",\n",
    "        \"answer\": \"China\",\n",
    "        \"aliases\": [\"China\", \"People's Republic of China\", \"PRC\"],\n",
    "        \"paraphrases\": [\n",
    "            \"Which nation tops global installed offshore wind capacity in 2024?\",\n",
    "            \"Who leads the world in offshore wind installations going into 2024?\",\n",
    "            \"Identify the country with the most offshore wind megawatts in 2024.\"\n",
    "        ],\n",
    "        \"context\": \"By 2024 China has the world's largest installed offshore wind capacity.\"\n",
    "    },\n",
    "    {\n",
    "        \"canonical_question\": \"Which city is home to the International Renewable Energy Agency headquarters?\",\n",
    "        \"answer\": \"Abu Dhabi\",\n",
    "        \"aliases\": [\"Abu Dhabi\", \"Abu Dhabi, UAE\", \"Abu Dhabi (UAE)\"],\n",
    "        \"paraphrases\": [\n",
    "            \"Which city hosts the headquarters of the International Renewable Energy Agency?\",\n",
    "            \"Where is IRENA's headquarters located?\",\n",
    "            \"Name the UAE city that houses the International Renewable Energy Agency.\"\n",
    "        ],\n",
    "        \"context\": \"The International Renewable Energy Agency (IRENA) maintains its headquarters in Abu Dhabi, United Arab Emirates.\"\n",
    "    },\n",
    "    {\n",
    "        \"canonical_question\": \"Which company operates the Hornsea offshore wind complex?\",\n",
    "        \"answer\": \"?rsted\",\n",
    "        \"aliases\": [\"?rsted\", \"Orsted\", \"Orsted A/S\"],\n",
    "        \"paraphrases\": [\n",
    "            \"Which company runs the Hornsea offshore wind farm complex?\",\n",
    "            \"Name the developer responsible for operating the Hornsea wind farms.\",\n",
    "            \"Who manages the Hornsea offshore wind projects in the North Sea?\"\n",
    "        ],\n",
    "        \"context\": \"The Hornsea offshore wind farms are developed and operated by ?rsted.\"\n",
    "    },\n",
    "    {\n",
    "        \"canonical_question\": \"Which river valley hosts the Three Gorges Dam?\",\n",
    "        \"answer\": \"Yangtze River\",\n",
    "        \"aliases\": [\"Yangtze River\", \"Yangtze\", \"Chang Jiang\"],\n",
    "        \"paraphrases\": [\n",
    "            \"The Three Gorges Dam spans which river valley?\",\n",
    "            \"Identify the river valley home to the Three Gorges Dam.\",\n",
    "            \"Which river does the Three Gorges Dam harness?\"\n",
    "        ],\n",
    "        \"context\": \"China's Three Gorges Dam spans the Yangtze River valley.\"\n",
    "    },\n",
    "    {\n",
    "        \"canonical_question\": \"Which desert houses the Noor Ouarzazate solar complex?\",\n",
    "        \"answer\": \"Sahara Desert\",\n",
    "        \"aliases\": [\"Sahara Desert\", \"Sahara\", \"Moroccan Sahara\"],\n",
    "        \"paraphrases\": [\n",
    "            \"In which desert is the Noor Ouarzazate solar complex located?\",\n",
    "            \"Name the desert that hosts Morocco's Noor solar complex.\",\n",
    "            \"Which desert houses the Noor Ouarzazate CSP facility?\"\n",
    "        ],\n",
    "        \"context\": \"Morocco's Noor Ouarzazate solar complex is located on the edge of the Sahara Desert.\"\n",
    "    },\n",
    "    {\n",
    "        \"canonical_question\": \"Which country launched the Himawari-9 weather satellite?\",\n",
    "        \"answer\": \"Japan\",\n",
    "        \"aliases\": [\"Japan\", \"Japanese government\", \"Japan Meteorological Agency\"],\n",
    "        \"paraphrases\": [\n",
    "            \"Which country launched the Himawari-9 weather satellite?\",\n",
    "            \"Identify the nation responsible for placing Himawari-9 into orbit.\",\n",
    "            \"Himawari-9 was launched by which country's space agency?\"\n",
    "        ],\n",
    "        \"context\": \"The Himawari weather satellites are operated and launched by Japan's meteorological agency.\"\n",
    "    },\n",
    "    {\n",
    "        \"canonical_question\": \"Which U.S. state hosts the National Renewable Energy Laboratory?\",\n",
    "        \"answer\": \"Colorado\",\n",
    "        \"aliases\": [\"Colorado\", \"State of Colorado\", \"Colorado State\"],\n",
    "        \"paraphrases\": [\n",
    "            \"Which U.S. state is home to the National Renewable Energy Laboratory?\",\n",
    "            \"Identify the U.S. state where NREL is headquartered.\",\n",
    "            \"In which state can you find the National Renewable Energy Laboratory campus?\"\n",
    "        ],\n",
    "        \"context\": \"The U.S. National Renewable Energy Laboratory (NREL) is headquartered in Golden, Colorado.\"\n",
    "    },\n",
    "    {\n",
    "        \"canonical_question\": \"Which city hosted the COP28 climate summit in 2023?\",\n",
    "        \"answer\": \"Dubai\",\n",
    "        \"aliases\": [\"Dubai\", \"Dubai, UAE\", \"Dubai in the UAE\"],\n",
    "        \"paraphrases\": [\n",
    "            \"Which city hosted the COP28 climate summit in 2023?\",\n",
    "            \"Name the city that welcomed COP28 in 2023.\",\n",
    "            \"Where was the 2023 COP28 climate conference held?\"\n",
    "        ],\n",
    "        \"context\": \"The COP28 climate summit in 2023 was hosted in Dubai, United Arab Emirates.\"\n",
    "    },\n",
    "    {\n",
    "        \"canonical_question\": \"Which company built the battery gigafactory Northvolt Ett?\",\n",
    "        \"answer\": \"Northvolt\",\n",
    "        \"aliases\": [\"Northvolt\", \"Northvolt AB\", \"Northvolt Ett\"],\n",
    "        \"paraphrases\": [\n",
    "            \"Which company built the Northvolt Ett battery gigafactory?\",\n",
    "            \"Name the firm that constructed the Northvolt Ett plant in Sweden.\",\n",
    "            \"Who developed the Northvolt Ett battery factory?\"\n",
    "        ],\n",
    "        \"context\": \"The Northvolt Ett battery gigafactory in Sweden was built by the company Northvolt.\"\n",
    "    },\n",
    "    {\n",
    "        \"canonical_question\": \"Which sea hosts the Dogger Bank offshore wind project?\",\n",
    "        \"answer\": \"North Sea\",\n",
    "        \"aliases\": [\"North Sea\", \"The North Sea\"],\n",
    "        \"paraphrases\": [\n",
    "            \"The Dogger Bank offshore wind farm is located in which sea?\",\n",
    "            \"Identify the sea that hosts the Dogger Bank wind project.\",\n",
    "            \"Dogger Bank wind arrays operate in what sea?\"\n",
    "        ],\n",
    "        \"context\": \"The Dogger Bank offshore wind arrays are located in the North Sea.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "CANONICAL_SYNONYMS = {}\n",
    "QUESTION_TO_CANONICAL = {}\n",
    "QUESTION_CONTEXT = {}\n",
    "teacher_trace = []\n",
    "distillation_examples = []\n",
    "\n",
    "def _register_answer(answer, aliases):\n",
    "    canonical = answer.strip().lower()\n",
    "    synonyms = CANONICAL_SYNONYMS.setdefault(canonical, set())\n",
    "    synonyms.add(canonical)\n",
    "    for alias in aliases:\n",
    "        synonyms.add(alias.strip().lower())\n",
    "\n",
    "for entry in QUESTION_BANK:\n",
    "    _register_answer(entry[\"answer\"], entry.get(\"aliases\", []))\n",
    "    variations = [entry[\"canonical_question\"]] + entry.get(\"paraphrases\", [])\n",
    "    context = entry.get(\"context\", \"\").strip()\n",
    "    for variant in variations:\n",
    "        key = variant.strip().lower()\n",
    "        QUESTION_TO_CANONICAL[key] = entry[\"answer\"]\n",
    "        if context:\n",
    "            QUESTION_CONTEXT[key] = context\n",
    "        pred = cot_qa(question=variant)\n",
    "        teacher_trace.append({\n",
    "            \"question\": variant,\n",
    "            \"teacher_answer\": pred.answer,\n",
    "            \"reasoning\": pred.reasoning\n",
    "        })\n",
    "        distillation_examples.append(\n",
    "            dspy.Example(question=variant, answer=entry[\"answer\"]).with_inputs('question')\n",
    "        )\n",
    "\n",
    "print(f\"Generated {len(distillation_examples)} distillation pairs across {len(QUESTION_BANK)} canonical questions.\")\n",
    "print(\"Sample teacher trace:\")\n",
    "for sample in teacher_trace[:5]:\n",
    "    print(f\"- Q: {sample['question']}\")\n",
    "    print(f\"  Teacher A: {sample['teacher_answer']}\")\n",
    "    print(f\"  Reasoning: {sample['reasoning']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c344cec9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:06:44.512497Z",
     "iopub.status.busy": "2025-09-21T13:06:44.512497Z",
     "iopub.status.idle": "2025-09-21T13:07:41.394672Z",
     "shell.execute_reply": "2025-09-21T13:07:41.378598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline student accuracy without context: 0.00%\n",
      "Baseline student accuracy with context: 87.50%\n",
      "- Which country hosts the ITER fusion reactor construction site? -> France (canonical: france)\n",
      "- In which European country is the ITER tokamak being assembled? -> France (canonical: france)\n",
      "- Which nation hosts the construction of the ITER fusion reactor? -> France (canonical: france)\n",
      "- Where is the ITER fusion megaproject located? -> southern France (canonical: france)\n",
      "- What is the capital of the country that operates the ALMA observatory? -> Santiago (canonical: santiago)\n",
      "- Name the capital city of the nation that operates the ALMA observatory. -> Santiago (canonical: santiago)\n",
      "- What city is the capital of Chile, the country running ALMA? -> Santiago (canonical: santiago)\n",
      "- Which city is the capital of the country that owns the ALMA array? -> Santiago (canonical: santiago)\n",
      "- Which nation leads the world's installed offshore wind capacity in 2024? -> China (canonical: china)\n",
      "- Which nation tops global installed offshore wind capacity in 2024? -> China (canonical: china)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "INFERENCE_PROMPT = \"Question: {question}\\nAnswer in one short phrase:\"\n",
    "\n",
    "def build_inference_prompt(question: str, use_context: bool = True) -> str:\n",
    "    if use_context:\n",
    "        key = question.strip().lower()\n",
    "        context = QUESTION_CONTEXT.get(key)\n",
    "        if context:\n",
    "            return f\"Context: {context}\\n\" + INFERENCE_PROMPT.format(question=question)\n",
    "    return INFERENCE_PROMPT.format(question=question)\n",
    "\n",
    "def canonicalize_answer(text):\n",
    "    normalized = text.strip().lower()\n",
    "    for canonical, aliases in CANONICAL_SYNONYMS.items():\n",
    "        if normalized in aliases:\n",
    "            return canonical\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def compute_accuracy(model, tokenizer, examples, use_context: bool = True):\n",
    "    correct = 0\n",
    "    predictions = []\n",
    "\n",
    "    for example in examples:\n",
    "        prompt = build_inference_prompt(example.question, use_context=use_context)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        outputs = model.generate(**inputs, max_new_tokens=48)\n",
    "        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        predictions.append((example.question, prediction))\n",
    "\n",
    "        predicted_canonical = canonicalize_answer(prediction)\n",
    "        target_canonical = canonicalize_answer(example.answer)\n",
    "        if predicted_canonical == target_canonical:\n",
    "            correct += 1\n",
    "\n",
    "    return correct / len(examples), predictions\n",
    "\n",
    "\n",
    "base_model_name = \"google/flan-t5-small\"\n",
    "baseline_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "baseline_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)\n",
    "\n",
    "baseline_raw_accuracy, _ = compute_accuracy(\n",
    "    baseline_model,\n",
    "    baseline_tokenizer,\n",
    "    distillation_examples,\n",
    "    use_context=False\n",
    ")\n",
    "\n",
    "baseline_context_accuracy, baseline_predictions = compute_accuracy(\n",
    "    baseline_model,\n",
    "    baseline_tokenizer,\n",
    "    distillation_examples,\n",
    "    use_context=True\n",
    ")\n",
    "\n",
    "print(f\"Baseline student accuracy without context: {baseline_raw_accuracy:.2%}\")\n",
    "print(f\"Baseline student accuracy with context: {baseline_context_accuracy:.2%}\")\n",
    "for question, prediction in baseline_predictions[:10]:\n",
    "    print(f\"- {question} -> {prediction} (canonical: {canonicalize_answer(prediction)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7a85b11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:07:41.545703Z",
     "iopub.status.busy": "2025-09-21T13:07:41.512014Z",
     "iopub.status.idle": "2025-09-21T13:08:07.398933Z",
     "shell.execute_reply": "2025-09-21T13:08:07.368114Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a51b849a544326be7a078490f9511f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/48 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ual-laptop\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4007: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned student accuracy without context: 0.00%\n",
      "Fine-tuned student accuracy with context: 66.67%\n",
      "- Which country hosts the ITER fusion reactor construction site? -> France (canonical: france)\n",
      "- In which European country is the ITER tokamak being assembled? -> France (canonical: france)\n",
      "- Which nation hosts the construction of the ITER fusion reactor? -> France (canonical: france)\n",
      "- Where is the ITER fusion megaproject located? -> Cadarache (canonical: cadarache)\n",
      "- What is the capital of the country that operates the ALMA observatory? -> Chile (canonical: chile)\n",
      "- Name the capital city of the nation that operates the ALMA observatory. -> Chile (canonical: chile)\n",
      "- What city is the capital of Chile, the country running ALMA? -> Santiago (canonical: santiago)\n",
      "- Which city is the capital of the country that owns the ALMA array? -> Chile (canonical: chile)\n",
      "- Which nation leads the world's installed offshore wind capacity in 2024? -> China (canonical: china)\n",
      "- Which nation tops global installed offshore wind capacity in 2024? -> offshore (canonical: offshore)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "import os\n",
    "\n",
    "DISTILLATION_PROMPT = \"Question: {question}\\nTeacher reasoning: {reasoning}\\nAnswer in one short phrase:\"\n",
    "MODEL_DIR = \"ft_flan_t5_simpleqa\"\n",
    "\n",
    "\n",
    "def build_training_prompt(question: str, reasoning: str) -> str:\n",
    "    key = question.strip().lower()\n",
    "    context = QUESTION_CONTEXT.get(key)\n",
    "    base = DISTILLATION_PROMPT.format(question=question, reasoning=reasoning)\n",
    "    if context:\n",
    "        return f\"Context: {context}\\n\" + base\n",
    "    return base\n",
    "\n",
    "train_inputs = []\n",
    "train_targets = []\n",
    "for example, trace in zip(distillation_examples, teacher_trace):\n",
    "    reasoning = trace[\"reasoning\"].strip()\n",
    "    prompt = build_training_prompt(trace[\"question\"], reasoning)\n",
    "    train_inputs.append(prompt)\n",
    "    train_targets.append(example.answer)\n",
    "\n",
    "train_pairs = Dataset.from_dict({\n",
    "    \"input_text\": train_inputs,\n",
    "    \"target_text\": train_targets,\n",
    "})\n",
    "\n",
    "model_name = \"google/flan-t5-small\"\n",
    "ft_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "ft_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def preprocess(batch):\n",
    "    model_inputs = ft_tokenizer(batch[\"input_text\"], max_length=200, truncation=True, padding=\"max_length\")\n",
    "    with ft_tokenizer.as_target_tokenizer():\n",
    "        labels = ft_tokenizer(batch[\"target_text\"], max_length=48, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "training_dataset = train_pairs.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=train_pairs.column_names\n",
    ")\n",
    "collator = DataCollatorForSeq2Seq(ft_tokenizer, model=ft_model)\n",
    "\n",
    "warmup_steps = max(1, len(train_inputs))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_DIR,\n",
    "    num_train_epochs=30,\n",
    "    learning_rate=8e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    weight_decay=0.02,\n",
    "    warmup_steps=warmup_steps,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=[],\n",
    "    remove_unused_columns=False,\n",
    "    label_smoothing_factor=0.1,\n",
    "    seed=13\n",
    ")\n",
    "\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    trainer = Trainer(\n",
    "        model=ft_model,\n",
    "        args=training_args,\n",
    "        train_dataset=training_dataset,\n",
    "        tokenizer=ft_tokenizer,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model(MODEL_DIR)\n",
    "    ft_tokenizer.save_pretrained(MODEL_DIR)\n",
    "\n",
    "finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR)\n",
    "finetuned_tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "raw_accuracy, _ = compute_accuracy(\n",
    "    finetuned_model,\n",
    "    finetuned_tokenizer,\n",
    "    distillation_examples,\n",
    "    use_context=False\n",
    ")\n",
    "context_accuracy, finetuned_predictions = compute_accuracy(\n",
    "    finetuned_model,\n",
    "    finetuned_tokenizer,\n",
    "    distillation_examples,\n",
    "    use_context=True\n",
    ")\n",
    "\n",
    "print(f\"Fine-tuned student accuracy without context: {raw_accuracy:.2%}\")\n",
    "print(f\"Fine-tuned student accuracy with context: {context_accuracy:.2%}\")\n",
    "for question, prediction in finetuned_predictions[:10]:\n",
    "    print(f\"- {question} -> {prediction} (canonical: {canonicalize_answer(prediction)})\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "10f50bf330324a26a67f703e7cc66409": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e6aa72e7d82147c5ab9cc8faa2122480",
       "max": 48,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7184dbb54d314924bffbba3c2a533e79",
       "value": 48
      }
     },
     "168e946914cd472cbad3c23f77051c3c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "17ea1c83971441bfbb5f937b7bf84cde": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "503afc9d2b834735b087686f4bd7a2ea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5ea33f95f3f44625a8054c0a57d273e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_503afc9d2b834735b087686f4bd7a2ea",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_17ea1c83971441bfbb5f937b7bf84cde",
       "value": "Map:â€‡100%"
      }
     },
     "620eabaa55c146b48e3a809eade2565b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "63de5e72f5324a31ad7710a57a94eb32": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e4de02d66b7945259352326d8a72e4ef",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_168e946914cd472cbad3c23f77051c3c",
       "value": "â€‡48/48â€‡[00:00&lt;00:00,â€‡313.92â€‡examples/s]"
      }
     },
     "7184dbb54d314924bffbba3c2a533e79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d8a51b849a544326be7a078490f9511f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5ea33f95f3f44625a8054c0a57d273e7",
        "IPY_MODEL_10f50bf330324a26a67f703e7cc66409",
        "IPY_MODEL_63de5e72f5324a31ad7710a57a94eb32"
       ],
       "layout": "IPY_MODEL_620eabaa55c146b48e3a809eade2565b"
      }
     },
     "e4de02d66b7945259352326d8a72e4ef": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e6aa72e7d82147c5ab9cc8faa2122480": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
